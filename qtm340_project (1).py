# -*- coding: utf-8 -*-
"""QTM340 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T-MWHtNvV1V0-tW4JBf_HE1fX62WTkXm
"""

# import the dataset
import os
import json
import gzip
import pandas as pd
import numpy as np
import sklearn
from tqdm import tqdm

import torch
import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from scipy.stats import percentileofscore

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, precision_score, recall_score
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, roc_auc_score

import seaborn as sns
import matplotlib.pyplot as plt

nltk.download('punkt')
nltk.download('stopwords')

import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")

import random
import numpy as np
from tqdm import tqdm_notebook as tqdm
from collections import defaultdict
from collections import Counter
from gensim.models.ldamulticore import LdaMulticore
from gensim.test.utils import datapath

"""#### Import the dataset:
Reference: https://colab.research.google.com/drive/1Zv6MARGQcrBbLHyjPVVMZVnRWsRnVMpV#scrollTo=feWoOrmt4Tja
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/metaFiles2/meta_All_Beauty.json.gz
# !wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/All_Beauty.json.gz

# load beauty data
data = []
with gzip.open('All_Beauty.json.gz', 'r') as f:
    for l in tqdm(f):
        data.append(json.loads(l))

df = pd.DataFrame(data)
df.shape

df = df.drop(['reviewerID', 'reviewTime','asin', 'unixReviewTime', 'style', 'image'],axis = 1)

df.dropna(inplace=True)

df.head()

df['vote']= df['vote'].str.replace(',', '').astype(int)
df['vote'].describe()

bin_edges = [0,5,10,15,20,25,30,35]
plt.hist(df['vote'], bins=bin_edges)  # Adjust the number of bins as necessary
plt.title('Distribution of Votes')
plt.xlabel('Votes')
plt.ylabel('Frequency')
plt.show()

percentile = percentileofscore(df['vote'], 15, kind='rank')
percentile

df['target'] = np.where(df['vote'] > 15, 1, 0)

df.head()

# remove stopwords and perform tokenization
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))

    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    return tokens

df['processed_reviewText'] = df['reviewText'].apply(preprocess_text)

# Separate into positive and negative subsets
positive_subset = df[df['target'] == 1]
negative_subset = df[df['target'] == 0]

# Sample 10 positive and 90 negative entries
positive_sample = positive_subset.sample(n=500, random_state = 42)
negative_sample = negative_subset.sample(n=4500, random_state = 42)

# Combine the samples
combined_sample = pd.concat([positive_sample, negative_sample])

# If you want to shuffle the combined sample
df =  combined_sample.sample(frac=1).reset_index(drop=True)

df.shape

"""## EDA

### Check the distribution of ratings:

- Skew towards giving 5/5 ratings in the reviews.
"""

sns.countplot(x='overall', data=df)

percentage_counts = df['overall'].value_counts(normalize=True) * 100

# Plotting
ax = sns.countplot(x='overall', data=df)

# Annotate with percentages
for p in ax.patches:
    ax.annotate(f'{p.get_height()/len(df)*100:.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.show()

sns.histplot(df['vote'], bins=40, kde=False)
plt.title('Histogram of Vote Distribution')
plt.xlabel('Number of Votes')
plt.ylabel('Frequency')
plt.show()

ax = sns.countplot(x='target', data=df)
total = float(len(df))
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width() / 2., height + 0.1,
            f'{height:.0f}\n{height/total:.2%}', ha="center", va="center")

plt.title('Distribution of Overall Ratings in Beauty Products Reviews')
plt.show()

"""### Plotting Word Cloud in the Review Text:


I filtered out the stopwords to make the word cloud more meaningful. I also found that the most frequent words include functional words like 'word,' 'use,' 'used,' so I decided to filter these words out to focus on the actual sentiments and trends in the review data.


"""

from wordcloud import WordCloud, STOPWORDS

def plot_word_cloud(text_data):
    # Ensure text_data is a string and is not empty
    if not isinstance(text_data, str) or not text_data:
        raise ValueError("Input must be a non-empty string.")

    wordcloud = WordCloud(
        width=3000,
        height=2000,
        background_color='black',
        stopwords=STOPWORDS
    ).generate(text_data)

    fig = plt.figure(
        figsize=(10, 7),
        facecolor='k',
        edgecolor='k'
    )

    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.tight_layout(pad=0)
    plt.show()

# Concatenate all text data in the 'reviewText' column into a single string and plot

all_reviews_text = ' '.join(df['reviewText'].dropna())
plot_word_cloud(all_reviews_text)


words_to_filter = ['product', 'use', 'used']
for word in words_to_filter:
    all_reviews_text = all_reviews_text.replace(word, '')

print("---------------------------------------------------------------------------------------------------------------------------------")
plot_word_cloud(all_reviews_text)

"""### Explore the relationship between the review rating and the number of votes (helpfulness of the review):"""

df['vote'] = pd.to_numeric(df['vote'], errors='coerce').fillna(0).astype(int)
sns.scatterplot(x='overall', y='vote', data=df)
plt.xlabel('Overall Rating')
plt.ylabel('Number of Votes')

plt.yscale('log')
plt.yticks([1, 10, 100, 1000])
plt.show()

averages = df.groupby('overall')['vote'].mean()

# Scatter plot with light blue color
sns.scatterplot(x='overall', y='vote', data=df, color='blue', s=30, alpha=0.5)

# Plot the averages
averages.plot(x='overall', y='vote', kind='line', marker='x', linestyle='--', color='red', linewidth=2, markersize=8)

plt.xlabel('Overall Rating')
plt.ylabel('Number of Votes')
plt.yscale('log')
plt.yticks([1, 10, 100, 1000])

plt.show()

"""### Explore the relationship between the length of the review summary and the number of votes:"""

from nltk.tokenize import word_tokenize

df['review_text_length'] = df['reviewText'].apply(lambda x: len(word_tokenize(x)) if isinstance(x, str) else 0)

sns.regplot(x='review_text_length', y='vote', data=df, scatter_kws={'s': 10}, line_kws={'color': 'red'})
plt.title('Relationship between Review Length and Vote')
plt.xlabel('Review Text Length (in words)')
plt.ylabel('Number of Votes')
plt.show()

"""### K-Means Clustering"""

helpful_text = df[df['target']==1]['reviewText']

vectorizer = TfidfVectorizer (sublinear_tf=True,
                              max_features=500,
                              max_df=0.5,
                              min_df=5)

vectorizer.fit(helpful_text)
tf_idf_mat = vectorizer.transform(df.summary)

from sklearn.cluster import KMeans
import numpy as np

n_clusters = 10

kmeans = KMeans(
    n_clusters=n_clusters,
    max_iter=50,
    n_init=5,
    random_state=340
)

kmeans.fit(tf_idf_mat)

order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names_out()

for i in range(n_clusters):
    print(f"Cluster {i}: ", end="")
    for ind in order_centroids[i, :10]:
        print(f"{terms[ind]} ", end="")
    print()

"""### Testing out LDA on Helpful Reviews:"""

def tokenize_summaries (helpful_text):
  stop = set(stopwords.words ('english'))
  tokens = nltk.word_tokenize(helpful_text)
  tokens = [token.lower () for token in tokens if token.isalpha() and token not in stop]
  return tokens

filtered_tokens = helpful_text.apply (lambda x: tokenize_summaries (x))

import gensim
from gensim.models.ldamulticore import LdaMulticore
from gensim.test.utils import datapath

# Construct a dictionary of words from the corpus
dictionary = gensim.corpora.Dictionary(filtered_tokens)
print (f"Before filtering: {len (dictionary)}")

# Filter the dictionary to meet frequency thresholds
dictionary.filter_extremes(no_below=10,
                           no_above=0.5,
                           keep_n=10000)
print (f"After filtering: {len (dictionary)}")

# The map of wordids back the words
id2token = {id: token for token, id in dictionary.token2id.items()}

# Construction of the corpus in the format that gensim expects
corpus = [dictionary.doc2bow(doc) for doc in filtered_tokens]

lda = LdaMulticore(corpus, id2word=id2token, num_topics=10, passes=50, iterations=500)

topics = lda.show_topics(num_topics=10, num_words=10, log=True, formatted=False)

# We'll print the top words associated with each topic
for topic_num, topic_words in topics:
  topic_rep = " ".join ([f"{w}({p:.4f})" for w,p in topic_words])
  print (f"Topic {topic_num}: {topic_rep}")

unhelpful_text = df[df['target']==0]['reviewText']

def tokenize_summaries2 (unhelpful_text):
  stop = set(stopwords.words ('english'))
  tokens = nltk.word_tokenize(unhelpful_text)
  tokens = [token.lower () for token in tokens if token.isalpha() and token not in stop]
  return tokens

filtered_tokens2 = unhelpful_text.apply (lambda x: tokenize_summaries2 (x))
dictionary2 = gensim.corpora.Dictionary(filtered_tokens2)
print (f"Before filtering: {len (dictionary2)}")

# Filter the dictionary to meet frequency thresholds
dictionary2.filter_extremes(no_below=10,
                           no_above=0.5,
                           keep_n=10000)
print (f"After filtering: {len (dictionary2)}")

id2token = {id: token for token, id in dictionary2.token2id.items()}

# Construction of the corpus in the format that gensim expects
corpus2 = [dictionary2.doc2bow(doc) for doc in filtered_tokens2]
lda2 = LdaMulticore(corpus2, id2word=id2token, num_topics=10, passes=50, iterations=500)

topics2 = lda2.show_topics(num_topics=10, num_words=10, log=True, formatted=False)

# We'll print the top words associated with each topic
for topic_num, topic_words in topics2:
  topic_rep = " ".join ([f"{w}({p:.4f})" for w,p in topic_words])
  print (f"Topic {topic_num}: {topic_rep}")

"""### Frequency Analysis: TF-IDF scores in helpful v. not-helpful reviews"""

all_reviews = df['reviewText']
target_labels = df['target']

# Use TfidfVectorizer for helpful reviews
vectorizer_helpful = TfidfVectorizer(stop_words='english', max_features=1000)
tfidf_matrix_helpful = vectorizer_helpful.fit_transform(all_reviews[target_labels == 1])
feature_names_helpful = vectorizer_helpful.get_feature_names_out()
idf_weights_helpful = tfidf_matrix_helpful.mean(0).A[0]

# Use TfidfVectorizer for unhelpful reviews
vectorizer_unhelpful = TfidfVectorizer(stop_words='english', max_features=1000)
tfidf_matrix_unhelpful = vectorizer_unhelpful.fit_transform(all_reviews[target_labels == 0])
feature_names_unhelpful = vectorizer_unhelpful.get_feature_names_out()
idf_weights_unhelpful = tfidf_matrix_unhelpful.mean(0).A[0]  # Convert to array


# Create DataFrames for both helpful and unhelpful terms and their IDF weights
df_helpful = pd.DataFrame({'Term': feature_names_helpful, 'idf_weights': idf_weights_helpful})
df_unhelpful = pd.DataFrame({'Term': feature_names_unhelpful, 'idf_weights': idf_weights_unhelpful})

# Sort the DataFrames by IDF weights in descending order
df_helpful = df_helpful.sort_values(by='idf_weights', ascending=False)
df_unhelpful = df_unhelpful.sort_values(by='idf_weights', ascending=False)

# Print or analyze the top terms in helpful reviews
print("Top terms in helpful reviews:")
print(df_helpful.head())

# Print or analyze the top terms in unhelpful reviews
print("\nTop terms in unhelpful reviews:")
print(df_unhelpful.head())

"""### Dependency Parsing"""

for index, row in df.iterrows():
    review_text = row['reviewText']

    doc = nlp(review_text)

# Access Dependency Parse Information
#    for token in doc:
#       print(f"{token.text} ({token.dep_}) --> {token.head.text}")

for token in doc:
  print (token.text, token.pos_, token.ent_type_)

for token in doc:
  print(token.text, token.dep_)

"""### Count the occurrences of features and extract the most common one:"""

# Define the feature extraction function
def extract_features(doc):
    features = []

    for token in doc:
        if token.dep_ == 'nsubj' or token.dep_ == 'dobj':
            features.append(token.text.lower())

    return features

# Process the corpus and collect features
all_features = []

# Iterate over rows where 'target' is equal to 1
for index, row in df[df['target'] == 1].iterrows():
    review_text = row['reviewText']
    doc = nlp(review_text)
    features = extract_features(doc)
    all_features.extend(features)

# Count the occurrences of features
feature_counter = Counter(all_features)

# Get the top common features
top_common_features = feature_counter.most_common(10)  # Adjust the number as needed

print(top_common_features)

"""### Get the top common syntax-wise patterns:"""

# Define a function to extract syntax-wise patterns from a SpaCy doc
def extract_syntax_patterns(doc):
    syntax_patterns = []
    for i in range(1, len(doc)):
        pattern = f"{doc[i-1].pos_} - {doc[i].pos_}"
        syntax_patterns.append(pattern)
    return syntax_patterns

# Process each review and extract syntax-wise patterns
all_syntax_patterns = []
for review in unhelpful_text:
    doc = nlp(review)
    syntax_patterns = extract_syntax_patterns(doc)
    all_syntax_patterns.extend(syntax_patterns)

# Count the occurrences of syntax-wise patterns
syntax_patterns_counter = Counter(all_syntax_patterns)

# Get the top common syntax-wise patterns
top_common_syntax_patterns = syntax_patterns_counter.most_common(10)  # Adjust the number as needed

# Print the results
for pattern, count in top_common_syntax_patterns:
    print(f"{pattern}: {count}")

"""DET - NOUN:
this product: 42
a bit: 16
this stuff: 13
a lot: 13
the color: 12
the product: 11
the end: 10
this one: 10
the water: 8
a week: 7


"""

# Define a function to extract syntax-wise patterns involving adjectives and nouns from a SpaCy doc
def extract_adj_noun_patterns(doc):
    adj_noun_patterns = []

    for i, token in enumerate(doc[:-1]):  # Iterate up to the second-to-last token
        # Check if the current token is an adjective and the next one is a noun
        if token.pos_ == 'ADJ' and doc[i+1].pos_ == 'NOUN':
            pattern = f"{token.text} {doc[i+1].text}"
            adj_noun_patterns.append(pattern)

    return adj_noun_patterns

# Process each review and extract patterns involving adjectives and nouns
all_adj_noun_patterns = []
for review in unhelpful_text:
    doc = nlp(review)
    adj_noun_patterns = extract_adj_noun_patterns(doc)
    all_adj_noun_patterns.extend(adj_noun_patterns)

# Count the occurrences of patterns involving adjectives and nouns
adj_noun_patterns_counter = Counter(all_adj_noun_patterns)

# Get the top common patterns involving adjectives and nouns
top_adj_noun_patterns = adj_noun_patterns_counter.most_common(10)  # Adjust the number as needed

# Print the results
for pattern, count in top_adj_noun_patterns:
    print(f"{pattern}: {count}")

"""### Caclulate the correlation between POS taggings among 'helpful' reivews:"""

def calculate_pos_correlation(sentences):
    # Process each sentence and collect POS tags
    pos_tags_per_sentence = []
    all_pos_tags = set()

    for sentence in sentences:
        doc = nlp(sentence)
        pos_tags = [token.pos_ for token in doc]
        pos_tags_per_sentence.append(pos_tags)
        all_pos_tags.update(pos_tags)

    # Create a DataFrame with one-hot encoding for each POS tag
    df = pd.DataFrame(0, columns=list(all_pos_tags), index=range(len(sentences)))

    # Populate the DataFrame with POS tag counts
    for i, pos_tags in enumerate(pos_tags_per_sentence):
        for tag in pos_tags:
            df.at[i, tag] += 1

    # Calculate the correlation matrix
    correlation_matrix = df.corr()

    return correlation_matrix

correlation_matrix = calculate_pos_correlation(df[df['target']==1]['reviewText'])

# Display the correlation matrix
print(correlation_matrix)

"""### Calculate the correlation between dependency labels among 'helpful' reviews:"""

def calculate_dependency_correlation(sentences):
    # Process each sentence and collect dependency labels
    dependency_labels_per_sentence = []
    all_dependency_labels = set()

    for sentence in sentences:
        doc = nlp(sentence)
        dependency_labels = [token.dep_ for token in doc]
        dependency_labels_per_sentence.append(dependency_labels)
        all_dependency_labels.update(dependency_labels)

    # Create a DataFrame with one-hot encoding for each dependency label
    df = pd.DataFrame(0, columns=list(all_dependency_labels), index=range(len(sentences)))

    # Populate the DataFrame with dependency label counts
    for i, dependency_labels in enumerate(dependency_labels_per_sentence):
        for label in dependency_labels:
            df.at[i, label] += 1

    # Calculate the correlation matrix
    correlation_matrix = df.corr()

    return correlation_matrix

correlation_matrix = calculate_dependency_correlation(df[df['target']==1]['reviewText'])
print(correlation_matrix)

"""# Helpful Review Predictions

### TFIDF with MLP
"""

vectorizer = TfidfVectorizer()
vectorizer.fit(df['reviewText'])


feature_names = vectorizer.get_feature_names_out()
idf_values = vectorizer.idf_

word_idf_dict = dict(zip(feature_names, idf_values))

# Sort words by their IDF scores in descending order and select top N words
sorted_words_by_idf = sorted(word_idf_dict.items(), key=lambda x: x[1], reverse=True)
top_n = 5000
top_n_words = [word for word, idf in sorted_words_by_idf[:min(top_n, len(sorted_words_by_idf))]]

# Create a new TF-IDF Vectorizer considering only the top N words
top_n_vectorizer = TfidfVectorizer(vocabulary=top_n_words)

top_n_tfidf_matrix = top_n_vectorizer.fit_transform(df['reviewText'])
feature_vectors = top_n_tfidf_matrix.toarray()

X_train, X_test, y_train, y_test = train_test_split(feature_vectors, df['target'], stratify = df['target'], test_size=0.3)

mlp = MLPClassifier(hidden_layer_sizes=(500,200))
mlp.fit(X_train, y_train)

score = mlp.score(X_test, y_test)

y_pred_proba = mlp.predict_proba(X_test)[:, 1]  # Adjust index if needed
auc_score = roc_auc_score(y_test, y_pred_proba)
y_pred = mlp.predict(X_test)
f1 = f1_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

"""### Word2Vec"""

sentences = df['processed_reviewText'].tolist()
model_w2v = Word2Vec(sentences, vector_size=500, window=5, min_count=1, workers=4)
model_w2v.save("word2vec.model")

def review_to_vec(review, model):
    vecs = [model.wv[word] for word in review if word in model.wv]
    return np.sum(vecs, axis=0) if vecs else np.zeros(model.vector_size)

df['reviewVec'] = df['processed_reviewText'].apply(lambda x: review_to_vec(x, model_w2v))

"""### Word Embedding with SVM"""

from sklearn.svm import SVC

X_train, X_test, y_train, y_test = train_test_split(df['reviewVec'].tolist(), df['target'], stratify = df['target'], test_size=0.3, random_state=42)

svm = SVC(kernel='poly', probability = True, random_state = 42)
svm.fit(X_train, y_train)


score = svm.score(X_test, y_test)

y_pred_proba = svm.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

y_pred = svm.predict(X_test)
f1 = f1_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

from sklearn.linear_model import LogisticRegression

_train, X_test, y_train, y_test = train_test_split(df['reviewVec'].tolist(), df['target'], stratify = df['target'], test_size=0.3, random_state=42)

# Initialize Logistic Regression model
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

# Evaluate the model
score = log_reg.score(X_test, y_test)

y_pred_proba = log_reg.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

y_pred = log_reg.predict(X_test)
f1 = f1_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")

"""### Word Embedding with MLP

"""

X_train, X_test, y_train, y_test = train_test_split(df['reviewVec'].tolist(), df['target'])

mlp = MLPClassifier(hidden_layer_sizes=(500,200))
mlp.fit(X_train, y_train)

score = mlp.score(X_test, y_test)

y_pred_proba = mlp.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

y_pred = mlp.predict(X_test)
f1 = f1_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""### Aggregate Word Embeddings using TFIDF"""

df.head()

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(df['reviewText'])

word2idf = {word: idf for word, idf in zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_)}

def review_to_vec(review, model):
    weighted_vecs = [model.wv[word] * word2idf.get(word, 1.0) for word in review if word in model.wv]
    return np.mean(weighted_vecs, axis=0) if weighted_vecs else np.zeros(model.vector_size)

df['reviewVec'] = df['processed_reviewText'].apply(lambda x: review_to_vec(x, model_w2v))

def evaluate_model(mlp, X, y):
    # Evaluate the model
    score = mlp.score(X, y)

    y_pred_proba = mlp.predict_proba(X)[:, 1]
    auc_score = roc_auc_score(y, y_pred_proba)

    y_pred = mlp.predict(X)
    f1 = f1_score(y, y_pred)

    cm = confusion_matrix(y, y_pred)

    return score, auc_score, f1, cm

X_temp, X_test, y_temp, y_test = train_test_split(df['reviewVec'].tolist(), df['target'], test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

param_grid = {
    'hidden_layer_sizes': [(500,200,100), (1000,500,200), (500,200), (300,100), (100,),(1000,)],
    'activation': ['relu', 'tanh', 'logistic']
    # 'solver': ['adam', 'sgd'],
    # 'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05],
    # 'learning_rate':['constant', 'invscaling', 'adaptive']
}


mlp = MLPClassifier()
grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, scoring='f1_score', cv=3)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print the best parameters
print("Best Parameters found by GridSearchCV:")
print(grid_search.best_params_)

# Use the best estimator for further predictions
best_mlp = grid_search.best_estimator_


val_score, val_auc_score, val_f1, val_cm = evaluate_model(best_mlp, X_val, y_val)
print("Validation Set Evaluation:")
print(f"Accuracy: {val_score}")
print(f"AUC Score: {val_auc_score}")
print(f"F1 Score: {val_f1}")
print(f"Confusion Matrix:\n{val_cm}")

# Evaluate on Test Set
test_score, test_auc_score, test_f1, test_cm = evaluate_model(best_mlp, X_test, y_test)
print("\nTest Set Evaluation:")
print(f"Accuracy: {test_score}")
print(f"AUC Score: {test_auc_score}")
print(f"F1 Score: {test_f1}")
print(f"Confusion Matrix:\n{test_cm}")

# Evaluate on Test Set
test_score, test_auc_score, test_f1, test_cm = evaluate_model(mlp, X_test, y_test)
print("\nTest Set Evaluation:")
print(f"Accuracy: {test_score}")
print(f"AUC Score: {test_auc_score}")
print(f"F1 Score: {test_f1}")
print(f"Confusion Matrix:\n{test_cm}")

"""### Contextual Embedding"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install datasets
# pip install transformers
# pip install sentencepiece

### Import Models
from transformers import *

tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')
model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', output_hidden_states=True)
model.eval()

def corpus2contextualmat_averagedlayers (corpus, last_layers=4):
  """ Take the contextual embedding of any word as the average of the
      embeddings of the word from the last 4 layers.
  """
  embeddings = []

  for i in tqdm(range(0, len(corpus), 32)):
      curr_batch = list(corpus[i:i+32])

      encoded_input = tokenizer(curr_batch,
                                return_tensors='pt',
                                max_length=512,
                                truncation=True,
                                padding=True)

      with torch.no_grad():
          outputs = model(**encoded_input) #  output_hidden_states=True

      hidden_states = outputs.hidden_states[-last_layers:] # Tuple of tf.Tensor (one for the output of the embeddings + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

      all_embeddings = torch.stack(hidden_states)
      # print(all_embeddings.ndim)
      # print(all_embeddings.shape)
      # [number of layers, batch size, sequence length, hidden size]
      avg_hidden_states = torch.mean(all_embeddings, dim=0) # take mean accorss the layers

      batch_embeddings = avg_hidden_states[:,0,:]
      embeddings.append(batch_embeddings)

  embeddings = torch.cat(embeddings, dim=0)
  return embeddings

df.head()

random_state = 5

# Separate into positive and negative subsets
positive_subset = df[df['target'] == 1]
negative_subset = df[df['target'] == 0]

# # Sample 10 positive and 90 negative entries
positive_sample = positive_subset.sample(n=50, random_state = random_state)
negative_sample = negative_subset.sample(n=450, random_state = random_state)

# # Combine the samples
combined_sample = pd.concat([positive_sample, negative_sample])

# # If you want to shuffle the combined sample
sample =  combined_sample.sample(frac=1).reset_index(drop=True)

contextual_embedding = corpus2contextualmat_averagedlayers(sample['reviewText'].values)

embeddings_list = [embedding.numpy().tolist() for embedding in contextual_embedding]
sample['contextualEmbedding'] = embeddings_list

X_train, X_test, y_train, y_test = train_test_split(sample['contextualEmbedding'].tolist(), sample['target'], stratify = sample['target'], test_size=0.3, random_state=random_state)

param_grid = {
     'hidden_layer_sizes': [(200,200), (500,500), (500,200,100), (500,500,200,100), (1000, 500,200,100), (1000,1000,500,500)],
    'activation': ['relu', 'tanh', 'logistic'],
    'learning_rate_init': [0.001, 0.01, 0.1],
    'solver': ['adam', 'sgd'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate':['constant', 'invscaling', 'adaptive']
}


mlp = MLPClassifier(random_state=random_state)
grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, scoring='f1', cv=3)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print the best parameters
print("Best Parameters found by GridSearchCV:")
print(grid_search.best_params_)

# Use the best estimator for further predictions
best_mlp = grid_search.best_estimator_

y_pred_proba = best_mlp.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

y_pred = best_mlp.predict(X_test)
f1 = f1_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

y_pred_proba = best_mlp.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

y_pred = best_mlp.predict(X_test)
f1 = f1_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

cm = confusion_matrix(y_test, y_pred)

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Assuming df, corpus2contextualmat_averagedlayers, and other necessary functions are already defined

best_f1 = 0
best_random_state = None
best_cm = None
best_auc = None
best_params = None

for random_state in range(30):  # Change 10 to any number of iterations you want
    # Separate into positive and negative subsets
    positive_subset = df[df['target'] == 1]
    negative_subset = df[df['target'] == 0]

    # Sample positive and negative entries
    positive_sample = positive_subset.sample(n=100, random_state=random_state)
    negative_sample = negative_subset.sample(n=100, random_state=random_state)

    # Combine and shuffle the samples
    combined_sample = pd.concat([positive_sample, negative_sample]).sample(frac=1).reset_index(drop=True)

    # Feature extraction
    contextual_embedding = corpus2contextualmat_averagedlayers(combined_sample['reviewText'].values)
    embeddings_list = [embedding.numpy().tolist() for embedding in contextual_embedding]
    combined_sample['contextualEmbedding'] = embeddings_list

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(combined_sample['contextualEmbedding'].tolist(), combined_sample['target'], stratify=combined_sample['target'], test_size=0.3, random_state=random_state)

    # Define parameter grid and model
    param_grid = {
      'hidden_layer_sizes': [(200,200), (500,500), (500,200,100), (500,500,200,100), (1000, 500,200,100), (1000,1000,500,500)],
      'activation': ['relu', 'tanh', 'logistic']
      # 'learning_rate_init': [0.001, 0.01, 0.1],
      # 'solver': ['adam', 'sgd'],
      # 'alpha': [0.0001, 0.001, 0.01],
      # 'learning_rate':['constant', 'invscaling', 'adaptive']
    }

    mlp = MLPClassifier(random_state=random_state)
    grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, scoring='f1', cv=3)

    # Fit the grid search to the data
    grid_search.fit(X_train, y_train)

    # Use the best estimator for predictions
    best_mlp = grid_search.best_estimator_
    y_pred = best_mlp.predict(X_test)
    f1 = f1_score(y_test, y_pred)

    # Check if this is the best score
    print(f'Random state: {random_state}, F1 Score: {f1}')
    if f1 > best_f1:
        best_f1 = f1
        best_random_state = random_state
        best_cm = confusion_matrix(y_test, y_pred)
        best_auc = roc_auc_score(y_test, best_mlp.predict_proba(X_test)[:, 1])
        best_params = grid_search.best_params_

# Print the best results
print(f"Best F1 Score: {best_f1}")
print(f"Best Random State: {best_random_state}")
print(f"Confusion Matrix:\n{best_cm}")
print(f"AUC Score: {best_auc}")
print("Best Parameters found by GridSearchCV:")
print(best_params)

### Comparative study
contextual_embedding1 = corpus2contextualmat_averagedlayers(sample['summary'].values)
embeddings_list = [embedding.numpy().tolist() for embedding in contextual_embedding1]
sample['contextualEmbedding1'] = embeddings_list
X_train, X_test, y_train, y_test = train_test_split(sample['contextualEmbedding1'].tolist(), sample['target'], test_size=0.2, random_state=42)

param_grid = {
    'hidden_layer_sizes': [(500,200,100), (1000,500,200), (500,200), (300,100), (200,), (100,)],
    'activation': ['relu', 'tanh', 'logistic'],
    # 'solver': ['adam', 'sgd'],
    'alpha': [0.0001, 0.001, 0.01],
    # 'learning_rate':['constant', 'invscaling', 'adaptive']
}


mlp = MLPClassifier()
grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, scoring='f1', cv=3)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print the best parameters
print("Best Parameters found by GridSearchCV:")
print(grid_search.best_params_)

# Use the best estimator for further predictions
best_mlp = grid_search.best_estimator_

y_pred_proba = best_mlp.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

y_pred = best_mlp.predict(X_test)
f1 = f1_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

sample.head()

sample.dtypes

import lightgbm as lgb
X_train, X_test, y_train, y_test = train_test_split(contextual_embedding.numpy() , sample['target'], test_size=0.2, random_state=42)

param_grid = {
    # 'max_depth': [3, 5, 7],
    # 'boosting_type': ['gbdt', 'dart', 'goss']
}

# Create a LightGBM classifier
lgbm = lgb.LGBMClassifier()

# Set up Grid Search
grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring='f1', cv=3)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print the best parameters
print("Best Parameters found by GridSearchCV:")
print(grid_search.best_params_)

# Use the best estimator for further predictions
best_lgbm = grid_search.best_estimator_

y_pred_proba = best_lgbm.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

y_pred = best_lgbm.predict(X_test)
f1 = f1_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

X_train, X_test, y_train, y_test = train_test_split(contextual_embedding.numpy() , sample['target'], test_size=0.2, random_state=42)

# Initialize and train LightGBM model
lgb_classifier = lgb.LGBMClassifier()
lgb_classifier.fit(X_train, y_train)

# Make predictions
y_pred = lgb_classifier.predict(X_test)

# Evaluate the model
y_pred_proba = lgb_classifier.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

y_pred = lgb_classifier.predict(X_test)
f1 = f1_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_classifier.fit(X_train, y_train)

# Make predictions
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
auc_score = roc_auc_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

# Print evaluation metrics
print(f"Accuracy: {accuracy}")
print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

"""### Ada Boost"""

from sklearn.ensemble import AdaBoostClassifier
svc = SVC(probability=True, kernel='linear')

# Initialize AdaBoost with the base learner
ada_classifier = AdaBoostClassifier(base_estimator=svc, n_estimators=50, random_state=42)

# Train the model
ada_classifier.fit(X_train, y_train)

# Make predictions
y_pred = ada_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
auc_score = roc_auc_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

# Print evaluation metrics
print(f"Accuracy: {accuracy}")
print(f"AUC Score: {auc_score}")
print(f"F1 Score: {f1}")

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()


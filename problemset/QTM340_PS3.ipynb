{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvov2_j6u3h7"
      },
      "source": [
        "In this problem set, we'll do a deep dive with language models.\n",
        "\n",
        "Once again, you're free to execute the notebook on your personal environment, but I would strongly recommend using Google Colab. You can upload this notebook to Google colab by following the steps below.\n",
        "\n",
        "1. Open [colab.research.google.com](colab.research.google.com)\n",
        "2. Click on the upload tab\n",
        "3. Upload the .ipynb file by choosing the right file from your local disk\n",
        "\n",
        "\n",
        "**Submission instructions**\n",
        "\n",
        "1. When you're ready to submit, you'll save the notebook as QTM340-PS3-Firstname-Lastname.ipynb; for example, if your name is Harry Potter, save the file as `QTM340-PS3-Harry-Potter.ipynb`. This can be done in Google colab by editing the filename and then following File --> Download --> .ipynb\n",
        "\n",
        "2. Upload this file on canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQNmRAJoPJkM"
      },
      "source": [
        "**Objective**: In this notebook, you'll learn the following in a classification task:\n",
        "\n",
        "a. To use bag of words representation as predictors (1 point)\n",
        "\n",
        "b. To use static word representations as predictors (2 points)\n",
        "\n",
        "c. To use contextual word representations as predictors (3 points)\n",
        "\n",
        "d. Explain what are the strengths and weaknesses of each of the model (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_YcHjoCbOY1"
      },
      "source": [
        "Our task is to classify research papers to categories. We'll use the dataset hosted by [huggingface](https://huggingface.co/datasets/gfissore/arxiv-abstracts-2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup"
      ],
      "metadata": {
        "id": "H2pz6KAmCVfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all the required packages."
      ],
      "metadata": {
        "id": "cfvP8TSs28BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install datasets\n",
        "pip install transformers\n",
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "ZNchZuXcBmg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get all the libraries imported first."
      ],
      "metadata": {
        "id": "kJQpWpof3Jep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import gensim\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "metadata": {
        "id": "km6ydzFW-ltU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now download the dataset and clean it up.\n",
        "\n",
        "\n",
        "**Note** This may take a couple of minutes when you run the first time because the data will be downloaded."
      ],
      "metadata": {
        "id": "pKKTyKDtA1hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert2label (x):\n",
        "  best_cat = x[0].split()[0]\n",
        "  return best_cat.split ('.')[0]\n",
        "\n",
        "required_cats = ['math', 'cs', 'astro-ph', 'physics', 'quant-ph']\n",
        "dataset = load_dataset(\"gfissore/arxiv-abstracts-2021\", split='train')\n",
        "dataset = dataset.remove_columns (column_names=['submitter',\n",
        "                                                'authors',\n",
        "                                                'journal-ref',\n",
        "                                                'doi',\n",
        "                                                'report-no',\n",
        "                                                'comments',\n",
        "                                                'versions'])\n",
        "df_dataset = pd.DataFrame(dataset)\n",
        "df_dataset[\"cat\"] = df_dataset.categories.apply (lambda x:convert2label (x))\n",
        "original_df = df_dataset.copy (deep=True)\n",
        "df_dataset = original_df.query ('cat in @required_cats')\n",
        "\n",
        "# randomly pick 1500 examples\n",
        "df_dataset = df_dataset.sample (n=1500, random_state=42)"
      ],
      "metadata": {
        "id": "sOsyzu_6BcgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have two variables that are of interest: `original_df` which contains all the examples in the dataset and `df_dataset` which contains examples that belong only to some fixed categories (as defined in `required_cats`)"
      ],
      "metadata": {
        "id": "w5sBCmky-sdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create a train (80%), validate (10%) and test (10%) split for our dataset."
      ],
      "metadata": {
        "id": "42q26MkkFtMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split df_dataset into train, validate and test dataframes\n",
        "train_df, test_df = train_test_split (df_dataset,\n",
        "                                      train_size=0.9,\n",
        "                                      random_state=42)\n",
        "\n",
        "train_df, val_df = train_test_split (train_df,\n",
        "                                     train_size=80/90,\n",
        "                                     random_state=42)"
      ],
      "metadata": {
        "id": "T5bFmNt6OhnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Bag of Words classification\n",
        "\n",
        "We'll turn the title into bag of words features."
      ],
      "metadata": {
        "id": "yPqJTXUsqAhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a vectorizer and classifier\n",
        "vectorizer = CountVectorizer (input=\"content\",\n",
        "                              lowercase=True,\n",
        "                              min_df=5,\n",
        "                              max_df=0.75,\n",
        "                              max_features=1000)\n",
        "classifier = LogisticRegression (penalty=\"l2\",\n",
        "                                 C=0.1,\n",
        "                                 max_iter=1000)\n",
        "\n",
        "# Fit the entire dataset on the vectorizer;\n",
        "# effectively, this line extracts all the features\n",
        "vectorizer.fit (df_dataset[\"title\"])\n",
        "\n",
        "# Get the labels\n",
        "y_train = train_df[\"cat\"].values\n",
        "y_val = val_df[\"cat\"].values\n",
        "y_test = test_df[\"cat\"].values\n",
        "\n",
        "# Get the bag-of-words representation for each document\n",
        "X_bow_train = vectorizer.transform (train_df[\"title\"])\n",
        "X_bow_val = vectorizer.transform (val_df[\"title\"])\n",
        "X_bow_test = vectorizer.transform (test_df[\"title\"])\n",
        "\n",
        "# Now, let's fit the model\n",
        "classifier.fit (X_bow_train, y_train)\n",
        "\n",
        "# Use the trained classifier to do predictions\n",
        "yhat_bow_val = classifier.predict (X_bow_val)\n",
        "\n",
        "# Get the accuracy of the classifier\n",
        "print (f\"Accuracy in %: {100*accuracy_score (y_val, yhat_bow_val):.2f}\")\n",
        "\n",
        "# Get the classification report\n",
        "print (\"Classification report\")\n",
        "print (classification_report (y_val, yhat_bow_val))"
      ],
      "metadata": {
        "id": "Lzn5ox2WPxfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sanity check** The bag-of-words features are quite predictive of the type of paper (60% accuracy); in comparison, a majority-class classifier -- one that predicts \"math\" for all examples -- will perform at 33% accuracy."
      ],
      "metadata": {
        "id": "zcjGkTSVe-VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q1. Adapt the code above to find the best regularization hyperparameter (highest accuracy) of the classifier. Report the optimal parameter and write 2-3 sentences to interpret the optimal regularization parameter [0.5 points]\n",
        "\n",
        "You'll tune the following parameters\n",
        "\n",
        "- C: The regularization penalty. Try all the values from the set {0.001, 0.01, 0.1, 1.0, 10.0, 100.0}\n",
        "\n",
        "Note that you'll have to calculate the accuracy on the validation set (not on the test set). You can learn about regularization [here](https://en.wikipedia.org/wiki/Regularization_(mathematics)) and how it's controlled by looking over Scikit's API documentation of logistic regression [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ],
      "metadata": {
        "id": "52rqSz6SlkKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code in this cell.\n",
        "\n"
      ],
      "metadata": {
        "id": "YegSYOcDpY7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. For the best classifier from Q1, report the top 10 and the bottom 10 features for each class that are most and least predictive of the label, respectively. Give a brief explanation for why you see these features at the top and bottom. [0.5 points]\n",
        "\n",
        "You can obtain the top 10 features by sorting them based on the coefficients learned by the classifier."
      ],
      "metadata": {
        "id": "vqmI4Rkepexj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code in this cell\n",
        "\n"
      ],
      "metadata": {
        "id": "4sISLjjOp0v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Classification using type embeddings\n",
        "\n",
        "We'll now learn the embeddings of each word and then use these embeddings as features in the classification model. The embeddings are used using [doc2vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) which is a variation of word2vec that learns embeddings sensitive to the topic or some label for every sentence.  \n",
        "\n",
        "\n",
        "We'll learn the parameters of the embedding model (i.e. word embeddings) from the abstracts and then construct the document embedding for the titles."
      ],
      "metadata": {
        "id": "O_ogCqTDqHUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=100,\n",
        "                                      min_count=5,\n",
        "                                      epochs=15)\n",
        "\n",
        "def read_corpus(iterable, tokens_only=False):\n",
        "  for i, line in enumerate(iterable):\n",
        "    tokens = gensim.utils.simple_preprocess(line)\n",
        "    if tokens_only:\n",
        "      yield tokens\n",
        "    else:\n",
        "      # For training data, add tags\n",
        "      yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "# Create the corpus in each split\n",
        "train_corpus_abstracts = list(read_corpus(train_df[\"abstract\"].values))\n",
        "\n",
        "train_corpus_titles = list(read_corpus(train_df[\"title\"].values, tokens_only=True))\n",
        "val_corpus_titles = list(read_corpus(val_df[\"title\"].values, tokens_only=True))\n",
        "test_corpus_titles = list(read_corpus(test_df[\"title\"].values, tokens_only=True))\n",
        "\n",
        "model.build_vocab(train_corpus_abstracts)\n",
        "model.train(train_corpus_abstracts,\n",
        "            total_examples=model.corpus_count,\n",
        "            epochs=model.epochs)"
      ],
      "metadata": {
        "id": "vdRbHBA2TFJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use `model.infer_vector` to get the vector representation of any document"
      ],
      "metadata": {
        "id": "S3Q1OrsmeGaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q1. Get the document vectors for every document in the train set to form the training matrix. Similarly construct the validation matrix and test matrix from documents in the validation and test corpus, respectively. [0.5 points]\n",
        "\n",
        "Following is an example of how to use `model.infer_vector` function, which will return a single vector for the entire sequence."
      ],
      "metadata": {
        "id": "mCD7E38TedQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.infer_vector([\"physics\", \"is\", \"awesome\"])\n",
        "print (vector)"
      ],
      "metadata": {
        "id": "M4QIvnlNdbnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def corpus2staticmat (corpus:list, training=False) -> np.array:\n",
        "  \"\"\" The function will take a corpus i.e. a collection of documents\n",
        "      and get the embedding for each document.\n",
        "\n",
        "  :params:\n",
        "  corpus (list): The corpus is in the form of a list. Every item\n",
        "                 in the list is a document. If the training flag is set,\n",
        "                 then a document contains two properties: words and tags;\n",
        "                 if the flag is not set, then the document is simply\n",
        "                 a list of words.\n",
        "\n",
        "  training (bool): A boolean flag that indicates whether the data\n",
        "                   is training or non-traiing data\n",
        "\n",
        "  :returns:\n",
        "  embeddings (np.array): The embeddings for each document are\n",
        "                         rows in a matrix.\n",
        "  \"\"\"\n",
        "\n",
        "  embeddings = None\n",
        "  # Write your code below\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "GH4rNP-ap48S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_static_train = corpus2staticmat (train_corpus_titles, training=False)\n",
        "X_static_val = corpus2staticmat (val_corpus_titles, training=False)\n",
        "X_static_test = corpus2staticmat (test_corpus_titles, training=False)"
      ],
      "metadata": {
        "id": "48F1uvqah8ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q2. Find the best classifier using the embeddings features. Once again, you'll find the best hyperparameter (based on accuracy) for vector size. [0.5 points]\n",
        "\n",
        "The vector size is a parameter for the following function `gensim.models.doc2vec.Doc2Vec`.\n",
        "\n",
        "- vector_size: Try values from the following set {25, 50, 100, 200}"
      ],
      "metadata": {
        "id": "MkXk-JLzG8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's fit the model\n",
        "classifier.fit (X_static_train, y_train)\n",
        "\n",
        "# Use the trained classifier to do predictions\n",
        "y_static_val = classifier.predict (X_static_val)\n",
        "\n",
        "# Get the accuracy of the classifier\n",
        "print (f\"Accuracy in %: {100*accuracy_score (y_val, y_static_val):.2f}\")\n",
        "\n",
        "# Get the classification report\n",
        "print (\"Classification report\")\n",
        "print (classification_report (y_val, y_static_val))"
      ],
      "metadata": {
        "id": "GmpWed-0KZCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sanity check**: I get 58% accuracy on the validation set using 100 dimensional features, which isn't bad considering I only trained the word2vec model for 15 epochs. There is also scope for improvement especially in categories that are rare."
      ],
      "metadata": {
        "id": "uLag4OJaKyY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code below for tuning the vector size parameter\n",
        "\n"
      ],
      "metadata": {
        "id": "BWHd3hv-Fos7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Compare the best classifier using just the bag-of-words feature and the classifier using doc2vec features. Which one is better in terms of accuracy? Briefly explain why? [1 point]"
      ],
      "metadata": {
        "id": "vd29TKNDIR3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z26uV63OIjAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Using contextual embeddings\n",
        "\n",
        "Now we'll use the embeddings from a variation of BERT as features to the classifier.\n",
        "\n",
        "The variation we'll use is called [SciBERT](https://arxiv.org/abs/1903.10676), which is the BERT model trained on scientific data such as research papers."
      ],
      "metadata": {
        "id": "HOZtbo3nIoSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import *\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', output_hidden_states=True)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "L5J8n8HpKiwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have the SciBERT model loaded, we can get the contextual embeddings for any sentence in a number of ways. One way is to take the embedding for the [CLS] token from the last layer using the `last_hidden_state` property set.\n",
        "\n",
        "Note: In general if you want to access the embeddings at any hidden layer, we can access the `hidden_states` property which contains the token embeddings at every layer starting from bottom layer to the topmost layer.\n",
        "\n",
        "Here's how to get the embeddings for the CLS token in any sequence."
      ],
      "metadata": {
        "id": "R3UvoI7aMiFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  text = \"Our paper measures the effect of eating ice-cream on happiness\"\n",
        "  encoded_input = tokenizer(text, return_tensors='pt')\n",
        "  output = model(**encoded_input)\n",
        "\n",
        "  # The [CLS] token is added at the start of the sentence,\n",
        "  # which you can access by the token position 0\n",
        "  # (the first zero is because we have only one sentence)\n",
        "  print (output.last_hidden_state[0,0,:])\n",
        "  print (output.last_hidden_state[0,0,:].size())"
      ],
      "metadata": {
        "id": "JDDAaxZJMswr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code should print the embedding output and the size of the embedding."
      ],
      "metadata": {
        "id": "rKhDEqql5PDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q1. Adapt the code above to get the contextual embeddings for all the examples in train, validate and test sets [1 point]\n",
        "\n",
        "You have to be careful with BERT-like models because it starts to break if the input text after tokenization exceeds 512 wordpieces, so you want to set the following parameters when you're calling the tokenizer on the sequence:\n",
        "\n",
        "- max_length to 512\n",
        "- truncation to True\n",
        "- padding is True"
      ],
      "metadata": {
        "id": "PpQf1RI6QD9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def corpus2contextualmat (corpus, batch_size=32):\n",
        "  embeddings = None\n",
        "  # Your code below\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "IgLOqjWuRBL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's call the method that gives us the contextual embeddings as follows.\n",
        "\n",
        "\n",
        "Note: This could take some time because usually transformer models run fast on GPUs but we'll end up running everything on the CPU offered by Colab server.\n",
        "\n",
        "It takes roughly 7-8 mins to run the cell below."
      ],
      "metadata": {
        "id": "jQln32VynozR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_contextual_train = corpus2contextualmat (train_df[\"title\"].values)\n",
        "X_contextual_val = corpus2contextualmat (val_df[\"title\"].values)\n",
        "X_contextual_test = corpus2contextualmat (test_df[\"title\"].values)"
      ],
      "metadata": {
        "id": "JbCSejtZRauY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q2. Report the accuracy by using the contextual embeddings of the titles. [0.5 points]"
      ],
      "metadata": {
        "id": "Bv7hP_cFp3HK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code below\n"
      ],
      "metadata": {
        "id": "V6BNcyDWWbZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Instead of taking the contextual embeddings from the final layer, get the embeddings from the last 4 layers, avearge them and use them as features in the classifier. [1 point]\n",
        "\n",
        "As mentioned, you can access the embeddings from individual layers using the `hidden_states` property of the output."
      ],
      "metadata": {
        "id": "0xYMgeONpkaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus2contextualmat_averagedlayers (corpus, last_layers=4):\n",
        "  \"\"\" Take the contextual embedding of any word as the average of the\n",
        "      embeddings of the word from the last 4 layers.\n",
        "  \"\"\"\n",
        "  embeddings = None\n",
        "  # Your code below\n",
        "  pass"
      ],
      "metadata": {
        "id": "XW7uAf7RrtJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_contextualaverage_train = corpus2contextualmat_averagedlayers (train_df[\"title\"].values, last_layers=4)\n",
        "X_contextualaverage_val = corpus2contextualmat_averagedlayers (val_df[\"title\"].values, last_layers=4)\n",
        "X_contextualaverage_test = corpus2contextualmat_averagedlayers (test_df[\"title\"].values, last_layers=4)"
      ],
      "metadata": {
        "id": "MajYcX9qs4eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Report the accuracy of the model with the features constructed above [0.5 points]"
      ],
      "metadata": {
        "id": "SI7J-n9fTuoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code below\n",
        "\n"
      ],
      "metadata": {
        "id": "3lqP7BlftQYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFc7Xl-qtXoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Testing on unseen data\n",
        "\n",
        "Now you have three competing classifiers:\n",
        "\n",
        "(a) The most optimized classifier that uses bag-of-words features to predict the type of paper\n",
        "\n",
        "(b) The most optimized classifier that uses static word embeddings to predict the type of paper\n",
        "\n",
        "(c) The most optimized classifier that uses contextual word embeddings to predict the type of paper\n",
        "\n",
        "**Your turn**\n",
        "\n",
        "Q1. List 5 examples from the validation set that were misclassified by each of the classifiers. Explain in brief why the classifiers got the examples correct or incorrect. [0.5 points]\n",
        "\n",
        "In answering the above question, you may want to think about the strengths and weaknesses of each of the classifiers.\n",
        "\n",
        "Q2. Among the 3 competing classifiers, pick the one that has the highest accuracy. Use the classifiers output on the validation set to identify the true label that is misclassified the most. Report what is it misclassified as and explain in 2-3 sentences why this might be the case [1 point]\n",
        "\n",
        "Q3. Report the accuracy and F1 score of all the competing classifiers. [0.5 points]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8lR_cplOuAOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Extra credit [1 point]\n",
        "\n",
        "\n",
        "Create the best classifier. You should use logistic regression but are free to try any other way to improve the performance of your classifier. Some suggestions include:\n",
        "\n",
        "- Combine all features from the best models\n",
        "- Add additional features\n",
        "- Get pretrained embeddings from other models\n",
        "\n",
        "You should briefly explain what you did in building the classifier. The best 3 submissions will get the extra credit which will be evaluated on data other than the one provided to you."
      ],
      "metadata": {
        "id": "6UdVZLp2uDDk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
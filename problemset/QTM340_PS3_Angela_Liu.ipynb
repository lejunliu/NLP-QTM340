{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lejunliu/NLP-QTM340/blob/main/QTM340_PS3_Angela_Liu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvov2_j6u3h7"
      },
      "source": [
        "In this problem set, we'll do a deep dive with language models.\n",
        "\n",
        "Once again, you're free to execute the notebook on your personal environment, but I would strongly recommend using Google Colab. You can upload this notebook to Google colab by following the steps below.\n",
        "\n",
        "1. Open [colab.research.google.com](colab.research.google.com)\n",
        "2. Click on the upload tab\n",
        "3. Upload the .ipynb file by choosing the right file from your local disk\n",
        "\n",
        "\n",
        "**Submission instructions**\n",
        "\n",
        "1. When you're ready to submit, you'll save the notebook as QTM340-PS3-Firstname-Lastname.ipynb; for example, if your name is Harry Potter, save the file as `QTM340-PS3-Harry-Potter.ipynb`. This can be done in Google colab by editing the filename and then following File --> Download --> .ipynb\n",
        "\n",
        "2. Upload this file on canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQNmRAJoPJkM"
      },
      "source": [
        "**Objective**: In this notebook, you'll learn the following in a classification task:\n",
        "\n",
        "a. To use bag of words representation as predictors (1 point)\n",
        "\n",
        "b. To use static word representations as predictors (2 points)\n",
        "\n",
        "c. To use contextual word representations as predictors (3 points)\n",
        "\n",
        "d. Explain what are the strengths and weaknesses of each of the model (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_YcHjoCbOY1"
      },
      "source": [
        "Our task is to classify research papers to categories. We'll use the dataset hosted by [huggingface](https://huggingface.co/datasets/gfissore/arxiv-abstracts-2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2pz6KAmCVfr"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfvP8TSs28BS"
      },
      "source": [
        "Install all the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNchZuXcBmg2",
        "outputId": "dd66a6b6-0b8c-4c5f-caae-f33ec2855011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip install datasets\n",
        "pip install transformers\n",
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJQpWpof3Jep"
      },
      "source": [
        "Let's get all the libraries imported first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km6ydzFW-ltU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import gensim\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKKTyKDtA1hU"
      },
      "source": [
        "Now download the dataset and clean it up.\n",
        "\n",
        "\n",
        "**Note** This may take a couple of minutes when you run the first time because the data will be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sOsyzu_6BcgU"
      },
      "outputs": [],
      "source": [
        "def convert2label (x):\n",
        "  best_cat = x[0].split()[0]\n",
        "  return best_cat.split ('.')[0]\n",
        "\n",
        "required_cats = ['math', 'cs', 'astro-ph', 'physics', 'quant-ph']\n",
        "dataset = load_dataset(\"gfissore/arxiv-abstracts-2021\", split='train')\n",
        "dataset = dataset.remove_columns (column_names=['submitter',\n",
        "                                                'authors',\n",
        "                                                'journal-ref',\n",
        "                                                'doi',\n",
        "                                                'report-no',\n",
        "                                                'comments',\n",
        "                                                'versions'])\n",
        "df_dataset = pd.DataFrame(dataset)\n",
        "df_dataset[\"cat\"] = df_dataset.categories.apply (lambda x:convert2label (x))\n",
        "original_df = df_dataset.copy (deep=True)\n",
        "df_dataset = original_df.query ('cat in @required_cats')\n",
        "\n",
        "# randomly pick 1500 examples\n",
        "df_dataset = df_dataset.sample (n=1500, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5sBCmky-sdR"
      },
      "source": [
        "You have two variables that are of interest: `original_df` which contains all the examples in the dataset and `df_dataset` which contains examples that belong only to some fixed categories (as defined in `required_cats`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42q26MkkFtMt"
      },
      "source": [
        "Next, we'll create a train (80%), validate (10%) and test (10%) split for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T5bFmNt6OhnW"
      },
      "outputs": [],
      "source": [
        "# Split df_dataset into train, validate and test dataframes\n",
        "train_df, test_df = train_test_split (df_dataset,\n",
        "                                      train_size=0.9,\n",
        "                                      random_state=42)\n",
        "\n",
        "train_df, val_df = train_test_split (train_df,\n",
        "                                     train_size=80/90,\n",
        "                                     random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPqJTXUsqAhx"
      },
      "source": [
        "## 1. Bag of Words classification\n",
        "\n",
        "We'll turn the title into bag of words features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lzn5ox2WPxfY",
        "outputId": "0f2c2324-366a-4990-bf29-07aa8ce89d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy in %: 60.00\n",
            "Classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    astro-ph       0.61      0.54      0.57        26\n",
            "          cs       0.65      0.56      0.60        39\n",
            "        math       0.56      0.92      0.69        49\n",
            "     physics       0.33      0.05      0.08        21\n",
            "    quant-ph       0.89      0.53      0.67        15\n",
            "\n",
            "    accuracy                           0.60       150\n",
            "   macro avg       0.61      0.52      0.52       150\n",
            "weighted avg       0.59      0.60      0.56       150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize a vectorizer and classifier\n",
        "vectorizer = CountVectorizer (input=\"content\",\n",
        "                              lowercase=True,\n",
        "                              min_df=5,\n",
        "                              max_df=0.75,\n",
        "                              max_features=1000)\n",
        "classifier = LogisticRegression (penalty=\"l2\",\n",
        "                                 C=0.1,\n",
        "                                 max_iter=1000)\n",
        "\n",
        "# Fit the entire dataset on the vectorizer;\n",
        "# effectively, this line extracts all the features\n",
        "vectorizer.fit (df_dataset[\"title\"])\n",
        "\n",
        "# Get the labels\n",
        "y_train = train_df[\"cat\"].values\n",
        "y_val = val_df[\"cat\"].values\n",
        "y_test = test_df[\"cat\"].values\n",
        "\n",
        "# Get the bag-of-words representation for each document\n",
        "X_bow_train = vectorizer.transform (train_df[\"title\"])\n",
        "X_bow_val = vectorizer.transform (val_df[\"title\"])\n",
        "X_bow_test = vectorizer.transform (test_df[\"title\"])\n",
        "\n",
        "# Now, let's fit the model\n",
        "classifier.fit (X_bow_train, y_train)\n",
        "\n",
        "# Use the trained classifier to do predictions\n",
        "yhat_bow_val = classifier.predict (X_bow_val)\n",
        "\n",
        "# Get the accuracy of the classifier\n",
        "print (f\"Accuracy in %: {100*accuracy_score (y_val, yhat_bow_val):.2f}\")\n",
        "\n",
        "# Get the classification report\n",
        "print (\"Classification report\")\n",
        "print (classification_report (y_val, yhat_bow_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcjGkTSVe-VD"
      },
      "source": [
        "**Sanity check** The bag-of-words features are quite predictive of the type of paper (60% accuracy); in comparison, a majority-class classifier -- one that predicts \"math\" for all examples -- will perform at 33% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52rqSz6SlkKV"
      },
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q1. Adapt the code above to find the best regularization hyperparameter (highest accuracy) of the classifier. Report the optimal parameter and write 2-3 sentences to interpret the optimal regularization parameter [0.5 points]\n",
        "\n",
        "You'll tune the following parameters\n",
        "\n",
        "- C: The regularization penalty. Try all the values from the set {0.001, 0.01, 0.1, 1.0, 10.0, 100.0}\n",
        "\n",
        "Note that you'll have to calculate the accuracy on the validation set (not on the test set). You can learn about regularization [here](https://en.wikipedia.org/wiki/Regularization_(mathematics)) and how it's controlled by looking over Scikit's API documentation of logistic regression [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YegSYOcDpY7J",
        "outputId": "031487ad-efb2-4500-dbb6-4a25b5a36ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy is %: 32.67\n",
            "Best C: 0.001\n",
            "Accuracy is %: 50.67\n",
            "Best C: 0.01\n",
            "Accuracy is %: 60.00\n",
            "Best C: 0.1\n",
            "Accuracy is %: 64.67\n",
            "Best C: 1.0\n",
            "Accuracy is %: 60.00\n",
            "Best C: 1.0\n",
            "Accuracy is %: 57.33\n",
            "Best C: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Your code in this cell.\n",
        "C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "best_C = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for C in C_values:\n",
        "  classifier = LogisticRegression(penalty=\"l2\",\n",
        "                                 C=C,\n",
        "                                 max_iter=1000)\n",
        "  classifier.fit (X_bow_train, y_train)\n",
        "  yhat_bow_val = classifier.predict(X_bow_val)\n",
        "  print (f\"Accuracy is %: {100*accuracy_score (y_val, yhat_bow_val):.2f}\")\n",
        "\n",
        "  accuracy = accuracy_score (y_val, yhat_bow_val)\n",
        "\n",
        "  if accuracy > best_accuracy:\n",
        "    best_accuracy = accuracy\n",
        "    best_C = C\n",
        "\n",
        "  print(f\"Best C: {best_C}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gbwzQeYnXDR"
      },
      "source": [
        "The optimal parameter is 1.0 with an accuracy of 64.67, which is a moderate value of C. Lower value of C means stronger regularization and prevents a lot of overfitting. Higher value of C, on the other hand, might be susceptible to overfitting while allowing more complex data. In this case, a C 0f 1.0, is not too high or too low, is a good balanced approach to regularization where it can be complex enough to capture the details but not that it would overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqmI4Rkepexj"
      },
      "source": [
        "Q2. For the best classifier from Q1, report the top 10 and the bottom 10 features for each class that are most and least predictive of the label, respectively. Give a brief explanation for why you see these features at the top and bottom. [0.5 points]\n",
        "\n",
        "You can obtain the top 10 features by sorting them based on the coefficients learned by the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4sISLjjOp0v3",
        "outputId": "381bc5e4-8759-4eb7-8ccc-6a1bc15e2482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class astro-ph:\n",
            "Top 10 predictive features:  ['planetary' 'gravitational' 'stellar' 'ray' 'cosmic' 'galactic'\n",
            " 'galaxies' 'cosmological' 'stars' 'observations']\n",
            "Bottom 10 predictive features:  ['quantum' 'electron' 'networks' 'to' 'graphs' 'via' 'random' 'on'\n",
            " 'identification' 'flow']\n",
            "Class cs:\n",
            "Top 10 predictive features:  ['codes' 'heterogeneous' 'feature' 'logic' 'data' 'social' 'recognition'\n",
            " 'learning' 'power' 'networks']\n",
            "Bottom 10 predictive features:  ['quantum' 'space' 'optical' 'atomic' 'gamma' 'decomposition' 'imaging'\n",
            " 'theorem' 'cosmological' 'noise']\n",
            "Class math:\n",
            "Top 10 predictive features:  ['decomposition' 'dimensional' 'operators' 'polynomials' 'spaces'\n",
            " 'algebras' 'forms' 'equations' 'conjecture' 'groups']\n",
            "Bottom 10 predictive features:  ['based' 'electron' 'channels' 'phase' 'networks' 'scale' 'entanglement'\n",
            " 'cosmological' 'magnetic' 'optical']\n",
            "Class physics:\n",
            "Top 10 predictive features:  ['turbulence' 'frequency' 'atomic' 'momentum' 'transport' 'ion' 'electron'\n",
            " 'scattering' 'pulses' 'photonic']\n",
            "Bottom 10 predictive features:  ['graphs' 'detection' 'applications' 'open' 'class' 'gravitational'\n",
            " 'large' 'spaces' 'infrared' 'system']\n",
            "Class quant-ph:\n",
            "Top 10 predictive features:  ['phase' 'non' 'single' 'uncertainty' 'entanglement' 'potential'\n",
            " 'channels' 'electron' 'between' 'quantum']\n",
            "Bottom 10 predictive features:  ['at' 'by' 'generation' 'groups' 'pulses' 'surface' 'for' 'real'\n",
            " 'learning' 'principle']\n"
          ]
        }
      ],
      "source": [
        "# Your code in this cell\n",
        "import numpy as np\n",
        "\n",
        "classifier = LogisticRegression(penalty=\"l2\",\n",
        "                                 C=1.0,\n",
        "                                 max_iter=1000)\n",
        "classifier.fit (X_bow_train, y_train)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "coefficients = classifier.coef_\n",
        "\n",
        "for i in range(len(required_cats)):\n",
        "    class_coefficients = coefficients[i]\n",
        "\n",
        "    sorted_coef_index = class_coefficients.argsort()\n",
        "    top_10 = sorted_coef_index[-10:]\n",
        "    bottom_10 = sorted_coef_index[:10]\n",
        "\n",
        "    top_10_features = feature_names[top_10]\n",
        "    bottom_10_features = feature_names[bottom_10]\n",
        "\n",
        "    print(f\"Class {classifier.classes_[i]}:\")\n",
        "    print(\"Top 10 predictive features: \", top_10_features)\n",
        "    print(\"Bottom 10 predictive features: \", bottom_10_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Gfid5kyE6_"
      },
      "source": [
        "In each class, the top 10 features are those with the highest positive coefficients for each class, indicating a strong positive correlation. On the other hand, the bottom 10 features are those with the most negative coefficients, indicating a strong negative correlation. It makes sense that for \"astro-ph\" class, words like stellar, cosmic, stars have the highest correlations while in the math class, words like magnetic and cosmological are least present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ogCqTDqHUc"
      },
      "source": [
        "## 2. Classification using type embeddings\n",
        "\n",
        "We'll now learn the embeddings of each word and then use these embeddings as features in the classification model. The embeddings are used using [doc2vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) which is a variation of word2vec that learns embeddings sensitive to the topic or some label for every sentence.  \n",
        "\n",
        "\n",
        "We'll learn the parameters of the embedding model (i.e. word embeddings) from the abstracts and then construct the document embedding for the titles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vdRbHBA2TFJ5"
      },
      "outputs": [],
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=100,\n",
        "                                      min_count=5,\n",
        "                                      epochs=15)\n",
        "\n",
        "def read_corpus(iterable, tokens_only=False):\n",
        "  for i, line in enumerate(iterable):\n",
        "    tokens = gensim.utils.simple_preprocess(line)\n",
        "    if tokens_only:\n",
        "      yield tokens\n",
        "    else:\n",
        "      # For training data, add tags\n",
        "      yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "# Create the corpus in each split\n",
        "train_corpus_abstracts = list(read_corpus(train_df[\"abstract\"].values))\n",
        "\n",
        "train_corpus_titles = list(read_corpus(train_df[\"title\"].values, tokens_only=True))\n",
        "val_corpus_titles = list(read_corpus(val_df[\"title\"].values, tokens_only=True))\n",
        "test_corpus_titles = list(read_corpus(test_df[\"title\"].values, tokens_only=True))\n",
        "\n",
        "model.build_vocab(train_corpus_abstracts)\n",
        "model.train(train_corpus_abstracts,\n",
        "            total_examples=model.corpus_count,\n",
        "            epochs=model.epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3Q1OrsmeGaI"
      },
      "source": [
        "Now use `model.infer_vector` to get the vector representation of any document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCD7E38TedQx"
      },
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q1. Get the document vectors for every document in the train set to form the training matrix. Similarly construct the validation matrix and test matrix from documents in the validation and test corpus, respectively. [0.5 points]\n",
        "\n",
        "Following is an example of how to use `model.infer_vector` function, which will return a single vector for the entire sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M4QIvnlNdbnJ",
        "outputId": "afa997a5-e500-4ad0-e5a8-5dcc7b10d5ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.00600176  0.04434367 -0.01625837  0.03682726  0.01159444 -0.06348134\n",
            "  0.01770834  0.11706414 -0.08930016 -0.01128201 -0.01944441 -0.04588564\n",
            "  0.00727504  0.0338801   0.01780108 -0.02409948  0.01820844 -0.02419796\n",
            "  0.00065879  0.00725534  0.03539961 -0.02332593  0.04525088 -0.01188623\n",
            "  0.00612918 -0.01086706 -0.02498062  0.03530251  0.0096279   0.00319705\n",
            "  0.07401841 -0.02162201  0.02130155 -0.03014926  0.0328822   0.02881766\n",
            "  0.01692638  0.03638084 -0.03389216 -0.02708544 -0.01894981 -0.02554624\n",
            " -0.06267942 -0.01599437 -0.01117519 -0.00456544  0.02475011  0.03130277\n",
            "  0.02778811  0.02143952 -0.00048141 -0.02061078  0.02850049  0.00278964\n",
            " -0.03652105  0.03064709  0.0083608  -0.02282735 -0.04796872 -0.01129916\n",
            "  0.01175171  0.01642023 -0.00480552  0.00549194 -0.05359496  0.03374572\n",
            "  0.03181355  0.05682414 -0.04113982  0.05748565 -0.02224613  0.02760835\n",
            "  0.03624192  0.02815029  0.05585455  0.02146997 -0.03445485  0.02921626\n",
            " -0.04448319 -0.05487447 -0.04940805  0.02785598 -0.00434917  0.0423071\n",
            " -0.0233355   0.00209713 -0.01536446  0.00877235  0.0426541   0.04602461\n",
            "  0.03848103 -0.00260233 -0.01925146 -0.02823259  0.09618176  0.0447159\n",
            " -0.00236796 -0.05673105  0.0411843   0.01885474]\n"
          ]
        }
      ],
      "source": [
        "vector = model.infer_vector([\"physics\", \"is\", \"awesome\"])\n",
        "print (vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GH4rNP-ap48S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def corpus2staticmat (corpus:list, training=False) -> np.array:\n",
        "  \"\"\" The function will take a corpus i.e. a collection of documents\n",
        "      and get the embedding for each document.\n",
        "\n",
        "  :params:\n",
        "  corpus (list): The corpus is in the form of a list. Every item\n",
        "                 in the list is a document. If the training flag is set,\n",
        "                 then a document contains two properties: words and tags;\n",
        "                 if the flag is not set, then the document is simply\n",
        "                 a list of words.\n",
        "\n",
        "  training (bool): A boolean flag that indicates whether the data\n",
        "                   is training or non-traiing data\n",
        "\n",
        "  :returns:\n",
        "  embeddings (np.array): The embeddings for each document are\n",
        "                         rows in a matrix.\n",
        "  \"\"\"\n",
        "\n",
        "  embeddings = []\n",
        "  # Write your code below\n",
        "  for doc in corpus:\n",
        "    if training:\n",
        "      doc_words = doc.words\n",
        "    else:\n",
        "      doc_words = doc\n",
        "\n",
        "    vector = model.infer_vector(doc_words)\n",
        "    embeddings.append(vector)\n",
        "\n",
        "  return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48F1uvqah8ur"
      },
      "outputs": [],
      "source": [
        "X_static_train = corpus2staticmat (train_corpus_titles, training=False)\n",
        "X_static_val = corpus2staticmat (val_corpus_titles, training=False)\n",
        "X_static_test = corpus2staticmat (test_corpus_titles, training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkXk-JLzG8ar"
      },
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q2. Find the best classifier using the embeddings features. Once again, you'll find the best hyperparameter (based on accuracy) for vector size. [0.5 points]\n",
        "\n",
        "The vector size is a parameter for the following function `gensim.models.doc2vec.Doc2Vec`.\n",
        "\n",
        "- vector_size: Try values from the following set {25, 50, 100, 200}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmpWed-0KZCX",
        "outputId": "55d828f0-f037-4bc7-d0f5-d92ef6e9f074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy in %: 62.67\n",
            "Classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    astro-ph       0.65      0.77      0.70        26\n",
            "          cs       0.62      0.67      0.64        39\n",
            "        math       0.62      0.92      0.74        49\n",
            "     physics       0.00      0.00      0.00        21\n",
            "    quant-ph       0.75      0.20      0.32        15\n",
            "\n",
            "    accuracy                           0.63       150\n",
            "   macro avg       0.53      0.51      0.48       150\n",
            "weighted avg       0.55      0.63      0.56       150\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Now, let's fit the model\n",
        "classifier.fit (X_static_train, y_train)\n",
        "\n",
        "# Use the trained classifier to do predictions\n",
        "y_static_val = classifier.predict (X_static_val)\n",
        "\n",
        "# Get the accuracy of the classifier\n",
        "print (f\"Accuracy in %: {100*accuracy_score (y_val, y_static_val):.2f}\")\n",
        "\n",
        "# Get the classification report\n",
        "print (\"Classification report\")\n",
        "print (classification_report (y_val, y_static_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLag4OJaKyY7"
      },
      "source": [
        "**Sanity check**: I get 58% accuracy on the validation set using 100 dimensional features, which isn't bad considering I only trained the word2vec model for 15 epochs. There is also scope for improvement especially in categories that are rare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWHd3hv-Fos7",
        "outputId": "d115e046-5cd5-467f-bb4c-c75290e6ea3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for vector size 25: 62.67%\n",
            "Accuracy for vector size 50: 62.67%\n",
            "Accuracy for vector size 100: 61.33%\n",
            "Accuracy for vector size 200: 58.67%\n",
            "Best vector size: 25 with accuracy: 62.67%\n"
          ]
        }
      ],
      "source": [
        "# Your code below for tuning the vector size parameter\n",
        "\n",
        "vector_sizes = [25,50, 100, 200]\n",
        "best_accuracy = 0\n",
        "best_vector_size = None\n",
        "best_classifier = None\n",
        "\n",
        "for vector_size in vector_sizes:\n",
        "  model = gensim.models.doc2vec.Doc2Vec(vector_size=vector_size,\n",
        "                                        min_count=5,\n",
        "                                        epochs=15)\n",
        "  model.build_vocab(train_corpus_abstracts)\n",
        "  model.train(train_corpus_abstracts,\n",
        "            total_examples=model.corpus_count,\n",
        "            epochs=model.epochs)\n",
        "\n",
        "  X_static_train = corpus2staticmat (train_corpus_titles, training=False)\n",
        "  X_static_val = corpus2staticmat (val_corpus_titles, training=False)\n",
        "  X_static_test = corpus2staticmat (test_corpus_titles, training=False)\n",
        "\n",
        "  classifier.fit(X_static_train, y_train)\n",
        "\n",
        "  y_static_val = classifier.predict(X_static_val)\n",
        "  accuracy = accuracy_score(y_val, y_static_val)\n",
        "  print(f\"Accuracy for vector size {vector_size}: {100*accuracy:.2f}%\")\n",
        "\n",
        "  if accuracy > best_accuracy:\n",
        "    best_accuracy = accuracy\n",
        "    best_vector_size = vector_size\n",
        "    best_classifier = classifier\n",
        "\n",
        "print(f\"Best vector size: {best_vector_size} with accuracy: {100*best_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd29TKNDIR3w"
      },
      "source": [
        "Q3. Compare the best classifier using just the bag-of-words feature and the classifier using doc2vec features. Which one is better in terms of accuracy? Briefly explain why? [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z26uV63OIjAS"
      },
      "source": [
        "Your answer here:\n",
        "\n",
        "The bag-of-words' best classifier has a higher accuracy comparing to the best classifier using doc2vec.\n",
        "One possible explanation for this is that in our dataset, the frequency of the words are good indicators of the class, so bag-of-words can capture it very effectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOZtbo3nIoSt"
      },
      "source": [
        "## 3. Using contextual embeddings\n",
        "\n",
        "Now we'll use the embeddings from a variation of BERT as features to the classifier.\n",
        "\n",
        "The variation we'll use is called [SciBERT](https://arxiv.org/abs/1903.10676), which is the BERT model trained on scientific data such as research papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5J8n8HpKiwn",
        "outputId": "5b7425ae-34f8-4250-ee22-c8761ac8ff4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/pytorch_model.bin\n",
            "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import *\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', output_hidden_states=True)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3UvoI7aMiFM"
      },
      "source": [
        "Once you have the SciBERT model loaded, we can get the contextual embeddings for any sentence in a number of ways. One way is to take the embedding for the [CLS] token from the last layer using the `last_hidden_state` property set.\n",
        "\n",
        "Note: In general if you want to access the embeddings at any hidden layer, we can access the `hidden_states` property which contains the token embeddings at every layer starting from bottom layer to the topmost layer.\n",
        "\n",
        "Here's how to get the embeddings for the CLS token in any sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDDAaxZJMswr",
        "outputId": "21ed0aa2-9e20-4525-d589-3f1955bee5f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-1.2158e+00,  2.5564e-01, -5.5524e-01,  5.1215e-01, -9.7400e-02,\n",
            "        -8.9400e-02,  6.6294e-01, -5.9423e-01, -6.3797e-01,  1.8451e-01,\n",
            "         3.7581e-01,  7.6952e-01, -6.3338e-01, -1.3066e-01, -8.0694e-01,\n",
            "        -4.9313e-02, -1.9207e+00,  4.9524e-01,  2.3647e-01, -4.6193e-01,\n",
            "         3.0016e-01, -8.6158e-01, -4.0208e-01, -2.6200e-01,  4.7555e-01,\n",
            "         6.2143e-01,  3.9351e-01, -3.8379e-01, -1.7755e-01,  3.0790e-01,\n",
            "         8.2291e-01, -1.3329e+00,  1.9409e-01, -8.8806e-01,  5.0629e-01,\n",
            "        -2.4628e-01, -2.9856e-02, -5.7507e-02, -7.9380e-02,  2.8860e-01,\n",
            "         9.3349e-02,  4.4720e-01,  3.9603e-01, -3.6464e-01, -2.7641e-03,\n",
            "        -7.4956e-01,  4.1467e-01,  7.9176e-01,  6.6163e-01, -1.1585e-01,\n",
            "         6.1243e-01,  6.4887e-01,  1.1093e+00,  4.6878e-01,  6.0221e-01,\n",
            "        -5.6090e-01, -5.2760e-01,  1.3664e-01,  4.9883e-01, -2.5344e-02,\n",
            "        -2.1232e-01, -2.5739e-01, -4.7110e-01,  1.3782e-01, -4.6313e-02,\n",
            "        -1.7934e-01,  5.0262e-01,  1.3339e-01, -2.9521e-02,  1.1096e+00,\n",
            "         7.9498e-01, -5.9193e-01, -4.3593e-01,  9.8135e-01,  3.3952e-01,\n",
            "        -1.2633e-01,  7.6456e-01,  2.7364e-02, -4.2277e-01, -1.6513e-01,\n",
            "         1.0836e+00, -4.4191e-01, -6.4880e-01, -1.8869e-01, -4.8879e-01,\n",
            "         1.1047e+00, -2.9128e-01,  9.2081e-01, -1.2090e-02,  9.6874e-02,\n",
            "         8.5376e-01,  7.7956e-01,  8.0025e-01, -2.3830e-01, -1.4685e-01,\n",
            "         3.0892e-01, -3.1328e-02, -9.2995e-02,  1.5261e-01,  8.3591e-01,\n",
            "        -5.1622e-01, -4.1725e-01,  3.9799e-01,  5.0110e-01,  7.0555e-01,\n",
            "         3.7715e-01,  2.0615e-01, -4.3297e-01, -2.4387e-01,  1.9287e-01,\n",
            "         7.7408e-02, -8.9667e-01, -6.0820e-01,  4.9635e-01,  8.0114e-01,\n",
            "         1.6254e-01, -3.0183e-01, -3.5929e-01, -7.2619e-01,  1.2862e+00,\n",
            "         1.0791e+00, -3.6811e-01, -4.6461e-02, -4.8484e-01,  4.8587e-01,\n",
            "         1.0925e+00, -5.2367e-01, -6.7458e-01,  2.2931e-01, -1.2338e+00,\n",
            "        -6.7410e-01, -1.8315e-01,  5.7977e-01,  3.6009e-01,  4.0814e-01,\n",
            "        -8.4377e-02, -8.1688e-01, -5.6324e-01,  3.1484e-01,  2.3625e-01,\n",
            "        -4.7532e-01,  1.9708e-01,  6.4821e-01,  1.0854e+00, -1.4739e-01,\n",
            "         6.5451e-01,  3.3563e-02, -1.3143e+00,  8.3531e-02, -5.1372e-01,\n",
            "        -4.9057e-01,  5.2602e-01, -3.0030e-01,  2.7904e-01, -4.9772e-01,\n",
            "        -6.4504e-01,  1.0632e+00, -6.2534e-01, -8.3716e-01,  2.0693e-01,\n",
            "         3.3973e-02, -7.5053e-01,  7.9118e-01, -1.3204e-01,  1.2286e-01,\n",
            "         2.7832e-01, -1.3765e-01,  9.7449e-03, -2.3769e-01, -1.6777e-01,\n",
            "        -6.1781e-01, -3.5208e-01,  6.5184e-02,  5.1481e-01,  5.1604e-01,\n",
            "        -3.3592e-01, -7.9140e-02, -1.1816e-02,  2.0852e-01,  2.8386e-02,\n",
            "        -1.2804e+00,  8.4947e-01,  3.6894e-01, -2.3713e-01, -2.8594e-01,\n",
            "        -5.3328e-01,  3.9196e-01, -4.1987e-01, -4.5108e-01, -9.0453e-01,\n",
            "         1.8378e-01, -2.0797e-01,  4.1157e-01, -3.1521e-01, -7.5731e-01,\n",
            "        -5.8058e-01, -3.9249e-01, -9.0895e-01, -3.0239e-01,  6.0886e-02,\n",
            "        -3.1600e-01, -1.8823e-02, -5.3199e-01, -1.4644e-01,  2.3191e-01,\n",
            "        -8.0072e-01, -1.9767e-01, -2.9525e-01,  5.1520e-01,  2.5694e-01,\n",
            "        -5.1732e-01, -2.3472e-01, -6.6604e-01, -3.5290e-01, -6.4083e-02,\n",
            "         1.1594e-01, -2.4676e-01, -7.1148e-01,  2.7231e-02,  9.0458e-01,\n",
            "         6.6308e-01, -3.3081e-01,  4.3914e-01, -2.4142e-01, -8.1394e-01,\n",
            "         1.0076e+00,  6.3537e-01, -1.7880e-01,  1.4595e-01,  1.4977e-01,\n",
            "         5.1152e-01, -1.2847e+00, -2.9251e-01,  4.4301e-01, -5.1745e-01,\n",
            "        -8.8760e-01, -7.0503e-01, -7.0593e-01,  3.4376e-01, -1.4332e-01,\n",
            "        -4.4236e-02,  5.7506e-01, -8.1255e-01, -3.4488e-01, -6.7048e-01,\n",
            "        -6.0651e-01, -2.2155e-01,  2.7096e-01,  5.5870e-01,  8.3832e-01,\n",
            "        -7.7926e-01, -3.6994e-01, -9.4622e-01,  3.8322e-01,  3.5830e-01,\n",
            "         1.7708e-01, -7.5376e-02, -4.5184e-01,  2.5686e-01, -9.4680e-01,\n",
            "        -1.7368e-01, -2.2394e+00,  7.8653e-01,  3.6750e-01, -5.1316e-01,\n",
            "         2.8665e-01, -6.5413e-01,  4.1554e-01,  5.4189e-01,  1.6800e-01,\n",
            "        -9.5539e-01, -6.0484e-02, -1.0651e-01, -1.1577e+00, -4.6940e-01,\n",
            "         2.6687e-01,  2.3492e-01, -5.5521e-01, -7.7127e-01, -9.9440e-01,\n",
            "         2.4864e-01,  7.6105e-01,  8.7161e-02,  6.3333e-01, -1.5837e-01,\n",
            "        -1.1089e+00,  3.5966e-01, -4.4060e-01, -7.4627e-01, -2.8679e-01,\n",
            "         1.6665e+00, -6.9185e-02,  7.8152e-01,  4.3828e-01, -3.6542e-01,\n",
            "        -7.8479e-01,  1.5740e-01,  5.5046e-01,  7.9917e-01,  7.2576e-01,\n",
            "         6.8386e-01,  3.1376e-02, -3.1759e-01,  5.2048e-01, -4.7258e-01,\n",
            "        -6.7818e-01,  8.6144e-02, -6.2441e-01, -8.6386e-01,  4.9023e-01,\n",
            "         8.7016e-01, -1.2067e+00, -8.4616e-01, -2.5603e-01, -2.2475e-01,\n",
            "         1.0502e-01, -6.0480e-01,  5.7923e-01, -7.3001e-01, -2.9752e-01,\n",
            "        -4.0482e-01,  4.3167e-01, -2.0581e-01, -1.5706e-01, -2.9277e-01,\n",
            "         5.6752e-01,  8.0634e-01,  3.6557e-01, -2.0065e-01, -3.7912e-01,\n",
            "        -6.8055e-01,  5.3020e-01,  1.5398e-01, -1.4464e-01,  5.3526e-01,\n",
            "         1.1319e-01, -2.9515e-01,  7.3404e-02,  1.1688e-01, -4.2847e-01,\n",
            "        -8.2497e-02,  9.2980e-01,  6.8112e-01, -6.6050e-01,  7.9052e-01,\n",
            "        -1.7056e-01, -9.9831e-01, -6.7935e-01, -2.5075e-01,  1.6975e-01,\n",
            "        -5.0009e-01, -2.0884e-02,  6.8157e-02, -5.5336e-01, -7.8809e-01,\n",
            "        -1.4274e-02, -5.3487e-01, -1.0050e+00,  4.7023e-01,  2.6695e-01,\n",
            "        -3.4057e-01, -7.7748e-01,  1.6022e-01,  2.1243e-01,  9.7489e-02,\n",
            "        -5.3167e-02, -3.9665e-01, -3.4321e-01,  1.7624e-01, -1.1490e-01,\n",
            "        -5.8017e-01, -7.2355e-02, -2.6944e-01,  6.7334e-02,  3.3431e-01,\n",
            "         1.2145e-01,  4.7694e-01, -5.2735e-01,  4.2560e-01,  1.8041e+00,\n",
            "         4.7000e-01, -6.0220e-02, -5.0156e-01, -1.0226e+00,  3.1301e-01,\n",
            "        -1.2727e+00,  1.9986e+00,  1.0206e+00, -1.8123e-01,  1.0392e+00,\n",
            "        -1.7406e-01,  9.5575e-02,  5.5721e-01, -2.7472e-01, -6.7445e-02,\n",
            "         4.7703e-01,  5.2681e-01,  1.3847e-01,  8.2677e-01,  4.9911e-01,\n",
            "         1.7395e-01, -7.7720e-01, -7.8040e-01,  1.5967e-01, -6.3341e-01,\n",
            "        -7.0237e-01,  5.4622e-01,  2.4691e-01,  3.1172e-01,  1.2957e-01,\n",
            "         5.5798e-01, -9.1162e-03,  9.6842e-03,  2.0349e-01,  6.5272e-01,\n",
            "        -5.0038e-01,  4.3148e-01,  3.6922e-01,  6.0812e-01,  1.4977e+00,\n",
            "         8.7390e-02,  8.4175e-01,  1.5749e+01, -3.8234e-01,  1.2365e-01,\n",
            "         3.7524e-01,  1.3055e-02,  2.5571e-01, -2.3464e-01,  3.8204e-01,\n",
            "        -3.8247e-01, -6.9950e-02,  3.7106e-02, -4.1615e-02,  3.7162e-01,\n",
            "         6.5461e-01,  9.0469e-01, -4.9217e-02, -8.6488e-01, -2.7318e-01,\n",
            "         3.5006e-01,  4.0199e-02,  3.4572e-01,  1.5353e+00,  2.7767e-02,\n",
            "        -2.5624e-01,  8.7148e-01, -1.6657e-01,  1.6838e-01, -3.0441e-01,\n",
            "         1.6149e-01, -2.3714e-01,  4.9059e-01, -7.4604e-01,  5.6718e-01,\n",
            "         1.2692e-01,  1.6032e-01, -6.9983e-01,  5.4148e-01, -8.9765e-01,\n",
            "        -1.0112e-01,  3.7262e-01,  5.8277e-01,  5.9130e-03,  1.4338e+00,\n",
            "        -1.6984e-01,  8.8860e-01,  6.2673e-01,  3.0044e-01,  7.7041e-01,\n",
            "         3.9947e-01, -1.4802e-01, -2.8810e-01,  6.4534e-01,  2.4931e-01,\n",
            "         2.1929e-01,  1.8516e-01,  3.4774e-01,  6.4490e-02, -1.7945e-01,\n",
            "        -1.1947e+00,  1.1088e+00, -6.5761e-01, -1.0344e+00,  1.4821e-01,\n",
            "        -4.4433e-01,  3.1755e-01, -2.4625e-01, -5.3716e-01, -6.8285e-02,\n",
            "         3.5806e-01, -7.2584e-01, -3.4153e-01, -4.5485e-01,  1.0876e-01,\n",
            "         2.9524e-01, -7.5220e-01, -5.5619e-02, -6.9879e-01,  2.3044e-01,\n",
            "        -3.0101e-01, -3.1959e-01,  4.8049e-01, -2.2684e-01,  1.4540e-01,\n",
            "        -2.1877e-01, -4.3824e-01,  1.4419e-01,  4.4139e-01, -9.5875e-02,\n",
            "        -4.1309e-01, -2.6893e-02, -7.4174e-01,  3.8663e-01,  1.6943e-02,\n",
            "        -8.4425e-03, -5.0671e-01,  1.1685e+00, -2.2657e-01,  9.2052e-02,\n",
            "        -5.2943e-02,  7.8898e-01,  2.3059e-01,  2.1562e-01,  7.3552e-01,\n",
            "        -2.1275e-01,  6.7496e-01,  2.7855e-01,  8.1923e-01, -4.1570e-01,\n",
            "        -1.0803e-01, -5.5544e-01,  4.9250e-01, -3.4178e-02,  3.3571e-01,\n",
            "        -1.1230e+00, -7.4824e-02, -1.3326e-01,  2.3600e-01,  1.2971e+00,\n",
            "        -9.5332e-02, -6.5307e-01, -4.4174e-01,  1.3861e-01,  3.8652e-01,\n",
            "        -1.0436e+00, -1.6962e-01, -5.2876e-01,  1.8481e-01, -3.4925e-01,\n",
            "        -8.6459e-01,  4.7279e-01, -4.5353e-02,  1.0593e-01,  1.5119e-01,\n",
            "         3.2207e-01,  2.7889e-01, -2.0461e-01,  3.8241e-01,  1.5055e-01,\n",
            "         1.6516e+00,  3.7915e-01, -1.5201e-01, -3.2715e-01, -1.1688e+00,\n",
            "        -4.6951e-01,  1.0540e+00,  3.6013e-01,  1.3377e-01, -3.5103e-01,\n",
            "         7.2890e-01,  4.6719e-01,  5.4569e-01, -5.9013e-01, -4.2558e-01,\n",
            "        -6.4739e-01,  1.1943e-01,  5.5715e-01,  3.4565e-01,  1.5546e+00,\n",
            "        -8.1592e-01, -1.2217e-01,  1.2680e+00, -6.8230e-02, -2.5638e-01,\n",
            "        -2.0924e-01,  4.6576e-01, -1.2260e+00, -5.8735e-01, -2.6730e-01,\n",
            "        -1.1406e-01, -1.0355e+00,  5.8070e-03,  2.7558e-01, -2.7676e-01,\n",
            "         2.0473e-01, -5.4796e-02,  3.0658e-01,  9.1909e-01, -3.4717e-01,\n",
            "         1.5651e-01, -4.6394e-02,  9.2926e-02, -1.2149e+00, -1.3023e-02,\n",
            "        -1.2695e-01,  7.6153e-01, -4.9133e-01, -7.8654e-01, -9.4668e-01,\n",
            "        -3.6157e-01,  2.1471e-01,  3.4337e-01,  4.4736e-01,  3.1603e-01,\n",
            "        -8.7447e-01, -2.8021e-01, -4.4919e-01,  4.0794e-02,  2.8078e-01,\n",
            "        -1.4311e-01, -7.7410e-02, -1.6868e-01, -3.3783e-01, -8.4479e-02,\n",
            "         3.1151e-02, -3.0365e-01, -1.2260e-01, -4.0477e-01,  3.4672e-01,\n",
            "         1.9167e-01, -1.7880e-01,  5.5624e-01,  1.0098e-01, -4.8993e-01,\n",
            "        -9.1503e-01, -9.5075e-01,  1.0278e-01, -4.5148e-02,  1.8048e-01,\n",
            "        -2.0559e-01, -6.0897e-01,  3.6383e-01, -7.0833e-01,  7.3751e-02,\n",
            "        -3.9110e-01,  4.9785e-01,  1.5013e-01, -4.4279e-01, -9.3521e-01,\n",
            "        -3.7657e-01, -6.9960e-01, -1.7594e-01, -9.4475e-01, -4.0828e-01,\n",
            "         1.8778e-01, -6.6778e-02, -6.2498e-03,  7.3503e-01,  1.3413e+00,\n",
            "        -2.3734e-01,  1.3789e+00,  6.4652e-01, -2.7286e-01,  3.1939e-01,\n",
            "        -1.6610e-02, -8.6943e-02, -9.3970e-02, -1.8635e-01, -1.1369e+00,\n",
            "        -8.7858e-01,  1.5432e-01,  3.7748e-01,  1.4229e+00, -6.7270e-01,\n",
            "        -7.8783e-01, -1.9965e-01, -7.4888e-01,  6.7368e-01, -1.0179e+00,\n",
            "        -3.3512e-01, -4.1630e-01, -9.8212e-01,  1.2766e+00,  4.5511e-01,\n",
            "        -2.5274e-01,  1.3414e-01,  1.6502e-01,  2.0259e-01,  1.2845e-01,\n",
            "         2.1296e-01, -1.1926e-01,  1.6991e-01, -3.0602e-01, -8.1488e-03,\n",
            "         4.1306e-02, -1.1688e-01,  1.0326e-01,  1.3913e+00, -3.9745e-01,\n",
            "         5.2542e-02, -5.5482e-01,  2.4758e-01,  1.9850e-01, -6.3738e-01,\n",
            "        -4.6022e-01,  5.8457e-01, -1.1309e+00, -6.0021e-01,  2.2331e-01,\n",
            "        -6.6998e-01, -4.9612e-03,  3.8015e-01,  4.9861e-02,  2.4100e-01,\n",
            "        -5.9576e-02, -3.2829e-01, -7.9649e-01,  9.0553e-01,  3.9929e-01,\n",
            "        -3.2661e-01,  9.4253e-01,  5.5747e-02,  7.4596e-01, -4.7865e-01,\n",
            "        -2.9041e-02,  2.8019e-01,  1.0223e-01,  1.5430e-01,  1.8874e-01,\n",
            "        -4.6075e-01,  1.2345e+00, -1.0103e+00,  6.1149e-01, -8.8629e-01,\n",
            "         1.3020e+00,  1.0630e-01,  6.6383e-01,  4.5406e-01, -1.1494e-01,\n",
            "         2.4751e-01,  9.4243e-01,  5.5260e-01,  5.2827e-01, -5.5878e-01,\n",
            "        -7.3413e-01,  4.8134e-01, -2.5903e-01, -1.0683e+00, -9.3943e-01,\n",
            "         3.2245e-02, -4.0555e-01, -6.1861e-02,  1.6332e-01,  7.9951e-03,\n",
            "         1.2421e+00,  5.0707e-02, -6.3788e-01,  2.9024e-01, -5.8328e-01,\n",
            "        -1.1633e+00, -1.6261e-01, -2.9307e-01,  3.1653e-03,  1.7803e-01,\n",
            "         5.3811e-01, -1.4870e+00, -9.9369e-01])\n",
            "torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  text = \"Our paper measures the effect of eating ice-cream on happiness\"\n",
        "  encoded_input = tokenizer(text, return_tensors='pt')\n",
        "  output = model(**encoded_input)\n",
        "\n",
        "  # The [CLS] token is added at the start of the sentence,\n",
        "  # which you can access by the token position 0\n",
        "  # (the first zero is because we have only one sentence)\n",
        "  print (output.last_hidden_state[0,0,:])\n",
        "  print (output.last_hidden_state[0,0,:].size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKhDEqql5PDT"
      },
      "source": [
        "The above code should print the embedding output and the size of the embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpQf1RI6QD9I"
      },
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q1. Adapt the code above to get the contextual embeddings for all the examples in train, validate and test sets [1 point]\n",
        "\n",
        "You have to be careful with BERT-like models because it starts to break if the input text after tokenization exceeds 512 wordpieces, so you want to set the following parameters when you're calling the tokenizer on the sequence:\n",
        "\n",
        "- max_length to 512\n",
        "- truncation to True\n",
        "- padding is True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgLOqjWuRBL9"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as ny\n",
        "\n",
        "def corpus2contextualmat(corpus, batch_size=32):\n",
        "    embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(corpus), 32)):\n",
        "        batch = list(corpus[i:i+32])\n",
        "        input = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            output = model(**input)\n",
        "\n",
        "        batch_emb = output.last_hidden_state[:, 0, :]\n",
        "        embeddings.append(batch_emb)\n",
        "\n",
        "    return torch.cat(embeddings,dim=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQln32VynozR"
      },
      "source": [
        "Now let's call the method that gives us the contextual embeddings as follows.\n",
        "\n",
        "\n",
        "Note: This could take some time because usually transformer models run fast on GPUs but we'll end up running everything on the CPU offered by Colab server.\n",
        "\n",
        "It takes roughly 7-8 mins to run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "JbCSejtZRauY",
        "outputId": "76866406-f860-4b6a-9aba-ce9fb175f451"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/38 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-377fc49fa988>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_contextual_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus2contextualmat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_contextual_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus2contextualmat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_contextual_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus2contextualmat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-aa4bfae55219>\u001b[0m in \u001b[0;36mcorpus2contextualmat\u001b[0;34m(corpus, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Doc2Vec' object is not callable"
          ]
        }
      ],
      "source": [
        "X_contextual_train = corpus2contextualmat (train_df[\"title\"].values)\n",
        "X_contextual_val = corpus2contextualmat (val_df[\"title\"].values)\n",
        "X_contextual_test = corpus2contextualmat (test_df[\"title\"].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv7hP_cFp3HK"
      },
      "source": [
        "**Your turn!**\n",
        "\n",
        "Q2. Report the accuracy by using the contextual embeddings of the titles. [0.5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6BNcyDWWbZH",
        "outputId": "e5ff06e2-551f-4ece-d5d4-334f33c42d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 80.00%\n"
          ]
        }
      ],
      "source": [
        "# Your code below\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit (X_contextual_train, y_train)\n",
        "\n",
        "y_predict = classifier.predict(X_contextual_val)\n",
        "accuracy = accuracy_score(y_val, y_predict)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xYMgeONpkaS"
      },
      "source": [
        "Q3. Instead of taking the contextual embeddings from the final layer, get the embeddings from the last 4 layers, avearge them and use them as features in the classifier. [1 point]\n",
        "\n",
        "As mentioned, you can access the embeddings from individual layers using the `hidden_states` property of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW7uAf7RrtJd"
      },
      "outputs": [],
      "source": [
        "def corpus2contextualmat_averagedlayers (corpus, last_layers=4):\n",
        "  \"\"\" Take the contextual embedding of any word as the average of the\n",
        "      embeddings of the word from the last 4 layers.\n",
        "  \"\"\"\n",
        "  embeddings = []\n",
        "  # Your code below\n",
        "  for i in tqdm(range(0, len(corpus), 32)):\n",
        "        batch = list(corpus[i:i+32])\n",
        "        input = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            output = model(**input)\n",
        "        hidden_states = output.hidden_states[-last_layers:]\n",
        "        avg_hidden_states = torch.mean(torch.stack(hidden_states), dim=0)\n",
        "        batch_emb = avg_hidden_states[:, 0, :]\n",
        "        embeddings.append(batch_emb)\n",
        "\n",
        "  return torch.cat(embeddings,dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MajYcX9qs4eJ",
        "outputId": "7c20fb83-a175-4e64-c2cb-f7b17310f226"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [02:44<00:00,  4.32s/it]\n",
            "100%|██████████| 5/5 [00:21<00:00,  4.31s/it]\n",
            "100%|██████████| 5/5 [00:21<00:00,  4.37s/it]\n"
          ]
        }
      ],
      "source": [
        "X_contextualaverage_train = corpus2contextualmat_averagedlayers (train_df[\"title\"].values, last_layers=4)\n",
        "X_contextualaverage_val = corpus2contextualmat_averagedlayers (val_df[\"title\"].values, last_layers=4)\n",
        "X_contextualaverage_test = corpus2contextualmat_averagedlayers (test_df[\"title\"].values, last_layers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI7J-n9fTuoH"
      },
      "source": [
        "Q4. Report the accuracy of the model with the features constructed above [0.5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lqP7BlftQYX",
        "outputId": "9ea3f4f9-8a7f-45b7-e3ad-5827b79e21ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 80.67%\n"
          ]
        }
      ],
      "source": [
        "# Your code below\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit (X_contextual_train, y_train)\n",
        "\n",
        "y_predict = classifier.predict(X_contextualaverage_val)\n",
        "accuracy = accuracy_score(y_val, y_predict)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lR_cplOuAOx"
      },
      "source": [
        "## 4. Testing on unseen data\n",
        "\n",
        "Now you have three competing classifiers:\n",
        "\n",
        "(a) The most optimized classifier that uses bag-of-words features to predict the type of paper\n",
        "\n",
        "(b) The most optimized classifier that uses static word embeddings to predict the type of paper\n",
        "\n",
        "(c) The most optimized classifier that uses contextual word embeddings to predict the type of paper\n",
        "\n",
        "**Your turn**\n",
        "\n",
        "Q1. List 5 examples from the validation set that were misclassified by each of the classifiers. Explain in brief why the classifiers got the examples correct or incorrect. [0.5 points]\n",
        "\n",
        "In answering the above question, you may want to think about the strengths and weaknesses of each of the classifiers.\n",
        "\n",
        "Q2. Among the 3 competing classifiers, pick the one that has the highest accuracy. Use the classifiers output on the validation set to identify the true label that is misclassified the most. Report what is it misclassified as and explain in 2-3 sentences why this might be the case [1 point]\n",
        "\n",
        "Q3. Report the accuracy and F1 score of all the competing classifiers. [0.5 points]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlEBEfx1PG1L"
      },
      "source": [
        "Q1:\n",
        "Strengths and weaknesses of each of the classifiers:\n",
        "\n",
        "#### bag-of-words\n",
        "1. Strength: Simple and effective by capturing the frequency of words making it strong classifier when keywords are good indicators.\n",
        "2. Weakness: ignores the context and order of words which might lead to losses and inaccurate interpretations of semantic meanings.\n",
        "3. Misclassification reasons: the words are representative of another class. Ex. \"electron\" are present a lot in physics so it got predicted as physics.\n",
        "\n",
        "#### static word embeddings\n",
        "1. Strength: Captures the sementic relationships of words\n",
        "2. Weakness: ignores the context\n",
        "3. Misclassification reasons: Words are very prevalent in other categories.\n",
        "\n",
        "#### contextual word embeddings\n",
        "1. Strength: Captures the dynamic word representations based on context and able to understand polysemy.\n",
        "2. Weakness: Longer run time and complex computations\n",
        "3. Misclassification reasons: It's interesting to see this classifier making really silly mistake such as : classifing \"The Transfer of Knowledge from Physics and Mathematics to Engineering Applications\" as CS where in fact it is physics. So, one drawback to this classifier is \"over-predicting\" when context is considered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M_zEv0WSV-v",
        "outputId": "b3df8159-5a12-48f3-e83f-39ef9bb7f457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 64.67%\n",
            "Title: On the maximum number of cliques in a graph\n",
            "\n",
            "Actual Label: math\n",
            "\n",
            "Predicted Label: cs\n",
            "\n",
            "Title: Upper Bounds of Interference Alignment Degree of Freedom\n",
            "\n",
            "Actual Label: cs\n",
            "\n",
            "Predicted Label: math\n",
            "\n",
            "Title: Polarization of Sunyaev-Zeldovich signal due to electron pressure\n",
            "  anisotropy in galaxy clusters\n",
            "\n",
            "Actual Label: astro-ph\n",
            "\n",
            "Predicted Label: physics\n",
            "\n",
            "Title: Minimal Degrees of Algebraic Numbers with respect to Primitive Elements\n",
            "\n",
            "Actual Label: math\n",
            "\n",
            "Predicted Label: cs\n",
            "\n",
            "Title: Beyond Fowler-Nordheim model: Harmonic generation from metallic\n",
            "  nano-structures\n",
            "\n",
            "Actual Label: physics\n",
            "\n",
            "Predicted Label: cs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bow = LogisticRegression(penalty=\"l2\", C=1.0, max_iter=1000)\n",
        "bow.fit(X_bow_train, y_train)\n",
        "\n",
        "yhat_bow_val = bow.predict(X_bow_val)\n",
        "accuracy = accuracy_score(y_val, yhat_bow_val)\n",
        "print(f\"Accuracy = {100*accuracy:.2f}%\")\n",
        "\n",
        "misclassified_count = 0\n",
        "for index, (predicted, actual) in enumerate(zip(yhat_bow_val, y_val)):\n",
        "    if predicted != actual:\n",
        "        print(f\"Title: {val_df['title'].iloc[index]}\\n\")\n",
        "        print(f\"Actual Label: {actual}\\n\")\n",
        "        print(f\"Predicted Label: {predicted}\\n\")\n",
        "        misclassified_count += 1\n",
        "        if misclassified_count >= 5:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udf6622rUZd-",
        "outputId": "67c63135-12b3-424e-b39d-52b13dbf177b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 62.00%\n",
            "Title: Upper Bounds of Interference Alignment Degree of Freedom\n",
            "\n",
            "Actual Label: cs\n",
            "\n",
            "Predicted Label: math\n",
            "\n",
            "Title: Linearized Reed-Solomon codes and linearized Wenger graphs\n",
            "\n",
            "Actual Label: cs\n",
            "\n",
            "Predicted Label: math\n",
            "\n",
            "Title: Pencil-Beam Single-point-fed Dirac Leaky-Wave Antenna on a\n",
            "  Transmission-Line Grid\n",
            "\n",
            "Actual Label: physics\n",
            "\n",
            "Predicted Label: astro-ph\n",
            "\n",
            "Title: Trapped surface formation for spherically symmetric\n",
            "  Einstein-Maxwell-charged scalar field system with double null foliation\n",
            "\n",
            "Actual Label: math\n",
            "\n",
            "Predicted Label: astro-ph\n",
            "\n",
            "Title: The Transfer of Knowledge from Physics and Mathematics to Engineering\n",
            "  Applications\n",
            "\n",
            "Actual Label: physics\n",
            "\n",
            "Predicted Label: cs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=5, epochs=15, seed=42)\n",
        "model.build_vocab(train_corpus_abstracts)\n",
        "model.train(train_corpus_abstracts, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "X_static_train = corpus2staticmat(train_corpus_titles, training=False)\n",
        "X_static_val = corpus2staticmat(val_corpus_titles, training=False)\n",
        "X_static_test = corpus2staticmat(test_corpus_titles, training=False)\n",
        "\n",
        "swb = LogisticRegression(penalty=\"l2\", max_iter=1000)\n",
        "swb.fit(X_static_train, y_train)\n",
        "\n",
        "yhat_static_val = swb.predict(X_static_val)\n",
        "accuracy = accuracy_score(y_val, yhat_static_val)\n",
        "print(f\"Accuracy = {100*accuracy:.2f}%\")\n",
        "\n",
        "misclassified_count = 0\n",
        "for index, (predicted, actual) in enumerate(zip(yhat_static_val, y_val)):\n",
        "    if predicted != actual:\n",
        "        print(f\"Title: {val_df['title'].iloc[index]}\\n\")\n",
        "        print(f\"Actual Label: {actual}\\n\")\n",
        "        print(f\"Predicted Label: {predicted}\\n\")\n",
        "\n",
        "        misclassified_count += 1\n",
        "        if misclassified_count >= 5:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zweX4QRwVMME",
        "outputId": "7878e7b1-a9a0-4ca0-adc9-f0c64f24c227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 82.00%\n",
            "Title: Endangered Languages are not Low-Resourced!\n",
            "\n",
            "Actual Label: cs\n",
            "\n",
            "Predicted Label: quant-ph\n",
            "\n",
            "Title: The Transfer of Knowledge from Physics and Mathematics to Engineering\n",
            "  Applications\n",
            "\n",
            "Actual Label: physics\n",
            "\n",
            "Predicted Label: cs\n",
            "\n",
            "Title: CSPEC: The cold chopper spectrometer of the European Spallation Source,\n",
            "  a detailed overview prior to commissioning\n",
            "\n",
            "Actual Label: physics\n",
            "\n",
            "Predicted Label: astro-ph\n",
            "\n",
            "Title: Unsourced Random Massive Access with Beam-Space Tree Decoding\n",
            "\n",
            "Actual Label: cs\n",
            "\n",
            "Predicted Label: quant-ph\n",
            "\n",
            "Title: Variable Modified Chaplygin Gas in Anisotropic Universe with\n",
            "  Kaluza-Klein Metric\n",
            "\n",
            "Actual Label: physics\n",
            "\n",
            "Predicted Label: math\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "cwb = LogisticRegression(penalty=\"l2\", max_iter=1000, random_state=42)\n",
        "cwb.fit(X_contextualaverage_train, y_train)\n",
        "\n",
        "yhat_contextualaverage_val = cwb.predict(X_contextualaverage_val)\n",
        "accuracy = accuracy_score(y_val, yhat_contextualaverage_val)\n",
        "print(f\"Accuracy = {100*accuracy:.2f}%\")\n",
        "\n",
        "misclassified_count = 0\n",
        "for index, (predicted, actual) in enumerate(zip(yhat_contextualaverage_val, y_val)):\n",
        "    if predicted != actual:\n",
        "        print(f\"Title: {val_df['title'].iloc[index]}\\n\")\n",
        "        print(f\"Actual Label: {actual}\\n\")\n",
        "        print(f\"Predicted Label: {predicted}\\n\")\n",
        "        misclassified_count += 1\n",
        "        if misclassified_count >= 5:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JX4vtPcPqH9"
      },
      "source": [
        "Q2:\n",
        "\n",
        "#### Highest accuracy: contextual word embedding with an accuracy of over 80 percent.\n",
        "\n",
        "#### Report the true label that is misclassified the most:\n",
        "\n",
        "Title: Endangered Languages are not Low-Resourced!\n",
        "\n",
        "Actual Label: cs\n",
        "\n",
        "Predicted Label: quant-ph\n",
        "\n",
        "Title: The Transfer of Knowledge from Physics and Mathematics to Engineering Applications\n",
        "\n",
        "Actual Label: physics\n",
        "\n",
        "Predicted Label: cs\n",
        "\n",
        "Title: CSPEC: The cold chopper spectrometer of the European Spallation Source, a detailed overview prior to commissioning\n",
        "\n",
        "Actual Label: physics\n",
        "\n",
        "Predicted Label: astro-ph\n",
        "\n",
        "Title: Unsourced Random Massive Access with Beam-Space Tree Decoding\n",
        "\n",
        "Actual Label: cs\n",
        "\n",
        "Predicted Label: quant-ph\n",
        "\n",
        "Title: Variable Modified Chaplygin Gas in Anisotropic Universe with Kaluza-Klein Metric\n",
        "\n",
        "Actual Label: physics\n",
        "\n",
        "Predicted Label: math\n",
        "\n",
        "#### Possible explanations:\n",
        "\n",
        "1. In The Transfer of Knowledge from Physics and Mathematics to Engineering Applications, the title includes terms like \"knowledge transfer\" and \"applications,\" which are often used in computer science contexts, so it causes the misclassification.\n",
        "2. In Unsourced Random Massive Access with Beam-Space Tree Decoding, it might be that the terms \"beam-space\" and \"decoding\" might be more frequently associated with quantum physics in the training data, leading to a misclassification.\n",
        "3. In Variable Modified Chaplygin Gas in Anisotropic Universe with Kaluza-Klein Metric, it contains specific math concepts like Kaluza-Klein Metric so it can lead to misclassification. However, the title also has the keyword \"gas\". But the classifier is unable to catch that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtdFatfuXl2_",
        "outputId": "58fb0559-6ea1-4aca-ac1b-40f7a179ad86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 74.67 , F1-Score: 0.74\n"
          ]
        }
      ],
      "source": [
        "yhat_bow_test = bow.predict(X_bow_test)\n",
        "F1 = f1_score(y_test, yhat_bow_test, average = 'weighted')\n",
        "accuracy = accuracy_score(y_test, yhat_bow_test)\n",
        "print(f\"Accuracy = {100*accuracy:.2f} , F1-Score: {F1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrdWxWU6XtQJ",
        "outputId": "f8b2eaaf-e362-4a79-b058-337f6a349f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 68.67 , F1-Score: 0.66\n"
          ]
        }
      ],
      "source": [
        "yhat_static_test = swb.predict(X_static_test)\n",
        "F1 = f1_score(y_test, yhat_static_test, average = 'weighted')\n",
        "accuracy = accuracy_score(y_test, yhat_static_test)\n",
        "print(f\"Accuracy = {100*accuracy:.2f} , F1-Score: {F1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOic7ugaXvIk",
        "outputId": "28ed2c81-061a-4555-c47a-71fec10f4454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 82.67 , F1-Score: 0.83\n"
          ]
        }
      ],
      "source": [
        "yhat_contextualaverage_test = cwb.predict(X_contextualaverage_test)\n",
        "F1 = f1_score(y_test, yhat_contextualaverage_test, average = 'weighted')\n",
        "accuracy = accuracy_score(y_test, yhat_contextualaverage_test)\n",
        "print(f\"Accuracy = {100*accuracy:.2f} , F1-Score: {F1:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

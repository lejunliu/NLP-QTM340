# -*- coding: utf-8 -*-
"""QTM340_Project_AmazonReviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KbL3-U0yzD0fMvsW4Ka16Bboylh_ulBy

# Introduction
In this colab, we present an example code snippet to **find target products** from the metadata we provide, e.g., based on the product titles.

In addition, we also show how to **align products with their reviews**, and find out the time spans that the products are on market based on their review times.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install spacy

import os
import json
import gzip
import pandas as pd
from urllib.request import urlopen

import random
import numpy as np
from tqdm import tqdm_notebook as tqdm
from collections import defaultdict
from gensim.models.ldamulticore import LdaMulticore
from gensim.test.utils import datapath

import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.metrics import mean_squared_error

import nltk
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')

import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")

!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Arts_Crafts_and_Sewing.json.gz
!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/All_Beauty.json.gz

# load beauty data
data = []
with gzip.open('All_Beauty.json.gz', 'r') as f:
    for l in tqdm(f):
        data.append(json.loads(l))

df = pd.DataFrame(data)
df.shape

df.dropna(inplace=True)

df.head()

df.shape
df.dtypes

df['vote']= df['vote'].str.replace(',', '').astype(int)
df['vote'].describe()

from scipy.stats import percentileofscore
percentile = percentileofscore(df['vote'], 30, kind='rank')
percentile

df['target'] = np.where(df['vote'] > 30, 1, 0)

df.head()

"""# Exploratory Data Analysis"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
import seaborn as sns
import matplotlib.pyplot as plt

df.head(3)
df

"""### Check the distribution of ratings:

- Skew towards giving 5/5 ratings in the reviews.
"""

sns.countplot(x='overall', data=df)

percentage_counts = df['overall'].value_counts(normalize=True) * 100

# Plotting
ax = sns.countplot(x='overall', data=df)

# Annotate with percentages
for p in ax.patches:
    ax.annotate(f'{p.get_height()/len(df)*100:.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Distribution of Overall Ratings in Beauty Products Reviews')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt


sns.histplot(df['vote'], bins=40, kde=False)
plt.title('Histogram of Vote Distribution')
plt.xlabel('Number of Votes')
plt.ylabel('Frequency')
plt.show()

ax = sns.countplot(x='target', data=df)
total = float(len(df))
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width() / 2., height + 0.1,
            f'{height:.0f}\n{height/total:.2%}', ha="center", va="center")

plt.title('Distribution of Overall Ratings in Beauty Products Reviews')
plt.show()

"""### Plotting Word Cloud in the Review Text:


I filtered out the stopwords to make the word cloud more meaningful. I also found that the most frequent words include functional words like 'word,' 'use,' 'used,' so I decided to filter these words out to focus on the actual sentiments and trends in the review data.


"""

from wordcloud import WordCloud, STOPWORDS

def plot_word_cloud(text_data):
    # Ensure text_data is a string and is not empty
    if not isinstance(text_data, str) or not text_data:
        raise ValueError("Input must be a non-empty string.")

    wordcloud = WordCloud(
        width=3000,
        height=2000,
        background_color='black',
        stopwords=STOPWORDS
    ).generate(text_data)

    fig = plt.figure(
        figsize=(10, 7),
        facecolor='k',
        edgecolor='k'
    )

    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.tight_layout(pad=0)
    plt.show()

# Concatenate all text data in the 'reviewText' column into a single string and plot

all_reviews_text = ' '.join(df['reviewText'].dropna())
plot_word_cloud(all_reviews_text)


words_to_filter = ['product', 'use', 'used']
for word in words_to_filter:
    all_reviews_text = all_reviews_text.replace(word, '')

print("---------------------------------------------------------------------------------------------------------------------------------")
plot_word_cloud(all_reviews_text)

"""### Explore the relationship between the review rating and the number of votes (helpfulness of the review):"""

df['vote'] = pd.to_numeric(df['vote'], errors='coerce').fillna(0).astype(int)
sns.scatterplot(x='overall', y='vote', data=df)
plt.title('Relationship between Vote and Overall Rating for Beauty Products')
plt.xlabel('Overall Rating')
plt.ylabel('Number of Votes')

plt.yscale('log')
plt.yticks([1, 10, 100, 1000])
plt.show()

"""### Explore the relationship between the length of the review summary and the number of votes:"""

from nltk.tokenize import word_tokenize

df['review_text_length'] = df['reviewText'].apply(lambda x: len(word_tokenize(x)) if isinstance(x, str) else 0)

sns.regplot(x='review_text_length', y='vote', data=df, scatter_kws={'s': 10}, line_kws={'color': 'red'})
plt.title('Relationship between Review Length and Vote')
plt.xlabel('Review Text Length (in words)')
plt.ylabel('Number of Votes')
plt.show()

"""# K-Means Clustering"""

df = df.drop(['reviewerID', 'reviewTime','asin', 'unixReviewTime', 'style', 'image'],axis = 1)

helpful_text = df[df['target']==1]['reviewText']

vectorizer = TfidfVectorizer (sublinear_tf=True,
                              max_features=500,
                              max_df=0.5,
                              min_df=5)

vectorizer.fit(helpful_text)
tf_idf_mat = vectorizer.transform(df.summary)

from sklearn.cluster import KMeans
import numpy as np

n_clusters = 10

kmeans = KMeans(
    n_clusters=n_clusters,
    max_iter=50,
    n_init=5,
    random_state=340
)

kmeans.fit(tf_idf_mat)

order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names_out()

for i in range(n_clusters):
    print(f"Cluster {i}: ", end="")
    for ind in order_centroids[i, :10]:
        print(f"{terms[ind]} ", end="")
    print()

"""# Testing out LDA:"""

def tokenize_summaries (reviewText):
  stop = set(stopwords.words ('english'))
  tokens = nltk.word_tokenize(reviewText)
  tokens = [token.lower () for token in tokens if token.isalpha() and token not in stop]
  return tokens

df['tokens'] = df['reviewText'].apply (lambda x: tokenize_summaries (x))

import gensim
from gensim.models.ldamulticore import LdaMulticore
from gensim.test.utils import datapath

# Construct a dictionary of words from the corpus
dictionary = gensim.corpora.Dictionary(df['tokens'])
print (f"Before filtering: {len (dictionary)}")

# Filter the dictionary to meet frequency thresholds
dictionary.filter_extremes(no_below=10,
                           no_above=0.5,
                           keep_n=10000)
print (f"After filtering: {len (dictionary)}")

# The map of wordids back the words
id2token = {id: token for token, id in dictionary.token2id.items()}

# Construction of the corpus in the format that gensim expects
corpus = [dictionary.doc2bow(doc) for doc in df['tokens']]

lda = LdaMulticore(corpus, id2word=id2token, num_topics=10, passes=5, iterations=200)

topics = lda.show_topics(num_topics=10, num_words=10, log=True, formatted=False)

# We'll print the top words associated with each topic
for topic_num, topic_words in topics:
  topic_rep = " ".join ([f"{w}({p:.4f})" for w,p in topic_words])
  print (f"Topic {topic_num}: {topic_rep}")

df.to_csv('df.csv', sep='\t')

"""## Dependency Parsing"""

for index, row in df.iterrows():
    review_text = row['reviewText']

    doc = nlp(review_text)

# Access Dependency Parse Information
#    for token in doc:
#       print(f"{token.text} ({token.dep_}) --> {token.head.text}")

for token in doc:
  print (token.text, token.pos_, token.ent_type_)

for token in doc:
  print(token.text, token.dep_)

"""## Count the occurrences of features and extract the most common one:"""

# Define the feature extraction function
def extract_features(doc):
    features = []

    for token in doc:
        if token.dep_ == 'nsubj' or token.dep_ == 'dobj':
            features.append(token.text.lower())

    return features

# Process the corpus and collect features
all_features = []

for index, row in df.iterrows():
    review_text = row['reviewText']
    doc = nlp(review_text)
    features = extract_features(doc)
    all_features.extend(features)

# Count the occurrences of features
feature_counter = Counter(all_features)

# Get the top common features
top_common_features = feature_counter.most_common(10)  # Adjust the number as needed

print(top_common_features)

"""## Get the top common syntax-wise patterns:"""

reviews = df['reviewText']

# Define a function to extract syntax-wise patterns from a SpaCy doc
def extract_syntax_patterns(doc):
    syntax_patterns = []
    for token in doc:
        # Customize this based on your specific patterns of interest
        pattern = f"{token.pos_} - {token.dep_}"

        # Append subtree information
        subtree = ' '.join([subtree.text for subtree in token.subtree])
        pattern += f" - Subtree: {subtree}"

        syntax_patterns.append(pattern)
    return syntax_patterns

# Process each review and extract syntax-wise patterns
all_syntax_patterns = []
for review in reviews:
    doc = nlp(review)
    syntax_patterns = extract_syntax_patterns(doc)
    all_syntax_patterns.extend(syntax_patterns)

# Count the occurrences of syntax-wise patterns
syntax_patterns_counter = Counter(all_syntax_patterns)

# Get the top common syntax-wise patterns
top_common_syntax_patterns = syntax_patterns_counter.most_common(10)  # Adjust the number as needed

# Print the results
for pattern, count in top_common_syntax_patterns:
    print(f"{pattern}: {count}")

"""## Caclulate the correlation between POS taggings among 'helpful' reivews:"""

def calculate_pos_correlation(sentences):
    # Process each sentence and collect POS tags
    pos_tags_per_sentence = []
    all_pos_tags = set()

    for sentence in sentences:
        doc = nlp(sentence)
        pos_tags = [token.pos_ for token in doc]
        pos_tags_per_sentence.append(pos_tags)
        all_pos_tags.update(pos_tags)

    # Create a DataFrame with one-hot encoding for each POS tag
    df = pd.DataFrame(0, columns=list(all_pos_tags), index=range(len(sentences)))

    # Populate the DataFrame with POS tag counts
    for i, pos_tags in enumerate(pos_tags_per_sentence):
        for tag in pos_tags:
            df.at[i, tag] += 1

    # Calculate the correlation matrix
    correlation_matrix = df.corr()

    return correlation_matrix

correlation_matrix = calculate_pos_correlation(df[df['target']==1]['reviewText'])

# Display the correlation matrix
print(correlation_matrix)

"""## Calculate the correlation between dependency labels among 'helpful' reviews:"""

def calculate_dependency_correlation(sentences):
    # Process each sentence and collect dependency labels
    dependency_labels_per_sentence = []
    all_dependency_labels = set()

    for sentence in sentences:
        doc = nlp(sentence)
        dependency_labels = [token.dep_ for token in doc]
        dependency_labels_per_sentence.append(dependency_labels)
        all_dependency_labels.update(dependency_labels)

    # Create a DataFrame with one-hot encoding for each dependency label
    df = pd.DataFrame(0, columns=list(all_dependency_labels), index=range(len(sentences)))

    # Populate the DataFrame with dependency label counts
    for i, dependency_labels in enumerate(dependency_labels_per_sentence):
        for label in dependency_labels:
            df.at[i, label] += 1

    # Calculate the correlation matrix
    correlation_matrix = df.corr()

    return correlation_matrix

correlation_matrix = calculate_dependency_correlation(df[df['target']==1]['reviewText'])
print(correlation_matrix)